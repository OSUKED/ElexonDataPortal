{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elexon Data Portal The ElexonDataPortal library is a Python Client for retrieving data from the Elexon/BMRS API. The library significantly reduces the complexity of interfacing with the Elexon/BMRS API through the standardisation of parameter names and orchestration of multiple queries when making requests over a date range. To use the ElexonDataPortal you will have to register for an Elexon API key which can be done here . Installation The library can be easily installed from PyPi, this can be done using: pip install ElexonDataPortal Getting Started We'll begin by initialising the API Client . The key parameter to pass here is the api_key , alternatively this can be set by specifying the environment variable BMRS_API_KEY which will then be loaded automatically. from ElexonDataPortal import api client = api.Client('your_api_key_here') Now that the client has been initialised we can make a request! One of the key abstractions within the ElexonDataPortal library is the handling of multiple requests over a date range specified through the start_date and end_date parameters. Each response will be automatically cleaned and parsed, then concatenated into a single Pandas DataFrame. If a settlement period and date column can be identified in the returned data then a new column will be added with the local datetime for each data-point. N.b. that if passed as a string the start and end datetimes will be assumed to be in the local timezone for the UK start_date = '2020-01-01' end_date = '2020-01-01 1:30' df_B1610 = client.get_B1610(start_date, end_date) df_B1610.head(3) documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId marketGenerationNGCBMUId bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity local_datetime 0 Actual generation Production Realised ELX-EMFIP-AGOG-TS-212 Sequential fixed size block 2020-01-01 Generation 48W000CAS-BEU01F 48W000CAS-BEU01F M_CAS-BEU01 CAS-BEU01 M_CAS-BEU01 CAS-BEU01 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 18.508 2020-01-01 00:00:00+00:00 1 Actual generation Production Realised ELX-EMFIP-AGOG-TS-355 Sequential fixed size block 2020-01-01 Generation 48W00000STLGW-3A 48W00000STLGW-3A T_STLGW-3 STLGW-3 T_STLGW-3 STLGW-3 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 28.218 2020-01-01 00:00:00+00:00 2 Actual generation Production Realised ELX-EMFIP-AGOG-TS-278 Sequential fixed size block 2020-01-01 Generation 48W00000GNFSW-1H 48W00000GNFSW-1H T_GNFSW-1 GNFSW-1 T_GNFSW-1 GNFSW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 29.44 2020-01-01 00:00:00+00:00 If you've previously written your own code for extracting data from the Elexon/BMRS API then you may be wondering where some of the normal parameters you pass have gone. The reduction in the parameters passed are due to 4 core drivers: Standardisation of date range parameter names Removal of the need to specify ServiceType Automatic passing of APIKey after client initialisation Shipped with sensible defaults for all remaining parameters The full list of data streams that are able to be requested can be found here . If you wish to make requests using the raw methods these are available through the ElexonDataportal.dev.raw module. Further information can be found in the Quick Start guide . What's Changed in v2 The latest release of the library includes a full rewrite of the code-base. We have endeavoured to make the new API as intuitive as possible but that has required breaking changes from v1, if you wish to continue using the historic library use pip install ElexonDataPortal==1.0.4 . N.b v1 will not be maintained going forward, you are advised to change over to v2.0.0+. The key feature changes are: Coverage of more BMRS streams Automated default values Cleaner client API A larger range of request types are compatible with the date range orchestrator Programmatic Library Generation One of the core features within the ElexonDataPortal library is that it is self-generating , by which we mean it can rebuild itself (including any new API request methods) from scratch using only the endpoints.csv spreadsheet. As well as generating the Python Client library a BMRS_API.yaml file is created, this provides an OpenAPI specification representation of the Elexon/BMRS API. In turn this allows us to automatically generate documentation, as well as run tests on the API itself to ensure that everything is working as expected - during this process we identified and corrected several small errors in the API documentation provided by Elexon. To rebuild the library simply run the following in the root directory: python -m ElexonDataPortal.rebuild N.b. If you wish to develop the library further or use any of the programmatic library generation functionality then please install the development version of the library using: pip install ElexonDataPortal[dev] If you are not installing into a fresh environment it is recommended you install pyyaml and geopandas using conda to avoid any dependency conflicts. In future we are looking to release ElexonDataPortal as a conda package to avoid these issues. Data Stream Descriptions The following table describes the data streams that are currently retreivable through the API. The client method to retrieve data from a given stream follows the naming convention get_{stream-name} . Stream Description B0610 Actual Total Load per Bidding Zone B0620 Day-Ahead Total Load Forecast per Bidding Zone B0630 Week-Ahead Total Load Forecast per Bidding Zone B0640 Month-Ahead Total Load Forecast Per Bidding Zone B0650 Year Ahead Total Load Forecast per Bidding Zone B0710 Planned Unavailability of Consumption Units B0720 Changes In Actual Availability Of Consumption Units B0810 Year Ahead Forecast Margin B0910 Expansion and Dismantling Projects B1010 Planned Unavailability In The Transmission Grid B1020 Changes In Actual Availability In The Transmission Grid B1030 Changes In Actual Availability of Offshore Grid Infrastructure B1320 Congestion Management Measures Countertrading B1330 Congestion Management Measures Costs of Congestion Management B1410 Installed Generation Capacity Aggregated B1420 Installed Generation Capacity per Unit B1430 Day-Ahead Aggregated Generation B1440 Generation forecasts for Wind and Solar B1510 Planned Unavailability of Generation Units B1520 Changes In Actual Availability of Generation Units B1530 Planned Unavailability of Production Units B1540 Changes In Actual Availability of Production Units B1610 Actual Generation Output per Generation Unit B1620 Actual Aggregated Generation per Type B1630 Actual Or Estimated Wind and Solar Power Generation B1720 Amount Of Balancing Reserves Under Contract Service B1730 Prices Of Procured Balancing Reserves Service B1740 Accepted Aggregated Offers B1750 Activated Balancing Energy B1760 Prices Of Activated Balancing Energy B1770 Imbalance Prices B1780 Aggregated Imbalance Volumes B1790 Financial Expenses and Income For Balancing B1810 Cross-Border Balancing Volumes of Exchanged Bids and Offers B1820 Cross-Border Balancing Prices B1830 Cross-border Balancing Energy Activated BOD Bid Offer Level Data CDN Credit Default Notice Data DETSYSPRICES Detailed System Prices DEVINDOD Daily Energy Volume Data DISBSAD Balancing Services Adjustment Action Data FORDAYDEM Forecast Day and Day Ahead Demand Data FREQ Rolling System Frequency FUELHH Half Hourly Outturn Generation by Fuel Type MELIMBALNGC Forecast Day and Day Ahead Margin and Imbalance Data MID Market Index Data MessageDetailRetrieval REMIT Flow - Message List Retrieval MessageListRetrieval REMIT Flow - Message List Retrieval NETBSAD Balancing Service Adjustment Data NONBM Non BM STOR Instructed Volume Data PHYBMDATA Physical Data SYSDEM System Demand SYSWARN System Warnings TEMP Temperature Data WINDFORFUELHH Wind Generation Forecast and Out-turn Data","title":"Welcome"},{"location":"#elexon-data-portal","text":"The ElexonDataPortal library is a Python Client for retrieving data from the Elexon/BMRS API. The library significantly reduces the complexity of interfacing with the Elexon/BMRS API through the standardisation of parameter names and orchestration of multiple queries when making requests over a date range. To use the ElexonDataPortal you will have to register for an Elexon API key which can be done here .","title":"Elexon Data Portal"},{"location":"#installation","text":"The library can be easily installed from PyPi, this can be done using: pip install ElexonDataPortal","title":"Installation"},{"location":"#getting-started","text":"We'll begin by initialising the API Client . The key parameter to pass here is the api_key , alternatively this can be set by specifying the environment variable BMRS_API_KEY which will then be loaded automatically. from ElexonDataPortal import api client = api.Client('your_api_key_here') Now that the client has been initialised we can make a request! One of the key abstractions within the ElexonDataPortal library is the handling of multiple requests over a date range specified through the start_date and end_date parameters. Each response will be automatically cleaned and parsed, then concatenated into a single Pandas DataFrame. If a settlement period and date column can be identified in the returned data then a new column will be added with the local datetime for each data-point. N.b. that if passed as a string the start and end datetimes will be assumed to be in the local timezone for the UK start_date = '2020-01-01' end_date = '2020-01-01 1:30' df_B1610 = client.get_B1610(start_date, end_date) df_B1610.head(3) documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId marketGenerationNGCBMUId bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity local_datetime 0 Actual generation Production Realised ELX-EMFIP-AGOG-TS-212 Sequential fixed size block 2020-01-01 Generation 48W000CAS-BEU01F 48W000CAS-BEU01F M_CAS-BEU01 CAS-BEU01 M_CAS-BEU01 CAS-BEU01 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 18.508 2020-01-01 00:00:00+00:00 1 Actual generation Production Realised ELX-EMFIP-AGOG-TS-355 Sequential fixed size block 2020-01-01 Generation 48W00000STLGW-3A 48W00000STLGW-3A T_STLGW-3 STLGW-3 T_STLGW-3 STLGW-3 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 28.218 2020-01-01 00:00:00+00:00 2 Actual generation Production Realised ELX-EMFIP-AGOG-TS-278 Sequential fixed size block 2020-01-01 Generation 48W00000GNFSW-1H 48W00000GNFSW-1H T_GNFSW-1 GNFSW-1 T_GNFSW-1 GNFSW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 29.44 2020-01-01 00:00:00+00:00 If you've previously written your own code for extracting data from the Elexon/BMRS API then you may be wondering where some of the normal parameters you pass have gone. The reduction in the parameters passed are due to 4 core drivers: Standardisation of date range parameter names Removal of the need to specify ServiceType Automatic passing of APIKey after client initialisation Shipped with sensible defaults for all remaining parameters The full list of data streams that are able to be requested can be found here . If you wish to make requests using the raw methods these are available through the ElexonDataportal.dev.raw module. Further information can be found in the Quick Start guide .","title":"Getting Started"},{"location":"#whats-changed-in-v2","text":"The latest release of the library includes a full rewrite of the code-base. We have endeavoured to make the new API as intuitive as possible but that has required breaking changes from v1, if you wish to continue using the historic library use pip install ElexonDataPortal==1.0.4 . N.b v1 will not be maintained going forward, you are advised to change over to v2.0.0+. The key feature changes are: Coverage of more BMRS streams Automated default values Cleaner client API A larger range of request types are compatible with the date range orchestrator","title":"What's Changed in v2"},{"location":"#programmatic-library-generation","text":"One of the core features within the ElexonDataPortal library is that it is self-generating , by which we mean it can rebuild itself (including any new API request methods) from scratch using only the endpoints.csv spreadsheet. As well as generating the Python Client library a BMRS_API.yaml file is created, this provides an OpenAPI specification representation of the Elexon/BMRS API. In turn this allows us to automatically generate documentation, as well as run tests on the API itself to ensure that everything is working as expected - during this process we identified and corrected several small errors in the API documentation provided by Elexon. To rebuild the library simply run the following in the root directory: python -m ElexonDataPortal.rebuild N.b. If you wish to develop the library further or use any of the programmatic library generation functionality then please install the development version of the library using: pip install ElexonDataPortal[dev] If you are not installing into a fresh environment it is recommended you install pyyaml and geopandas using conda to avoid any dependency conflicts. In future we are looking to release ElexonDataPortal as a conda package to avoid these issues.","title":"Programmatic Library Generation"},{"location":"#data-stream-descriptions","text":"The following table describes the data streams that are currently retreivable through the API. The client method to retrieve data from a given stream follows the naming convention get_{stream-name} . Stream Description B0610 Actual Total Load per Bidding Zone B0620 Day-Ahead Total Load Forecast per Bidding Zone B0630 Week-Ahead Total Load Forecast per Bidding Zone B0640 Month-Ahead Total Load Forecast Per Bidding Zone B0650 Year Ahead Total Load Forecast per Bidding Zone B0710 Planned Unavailability of Consumption Units B0720 Changes In Actual Availability Of Consumption Units B0810 Year Ahead Forecast Margin B0910 Expansion and Dismantling Projects B1010 Planned Unavailability In The Transmission Grid B1020 Changes In Actual Availability In The Transmission Grid B1030 Changes In Actual Availability of Offshore Grid Infrastructure B1320 Congestion Management Measures Countertrading B1330 Congestion Management Measures Costs of Congestion Management B1410 Installed Generation Capacity Aggregated B1420 Installed Generation Capacity per Unit B1430 Day-Ahead Aggregated Generation B1440 Generation forecasts for Wind and Solar B1510 Planned Unavailability of Generation Units B1520 Changes In Actual Availability of Generation Units B1530 Planned Unavailability of Production Units B1540 Changes In Actual Availability of Production Units B1610 Actual Generation Output per Generation Unit B1620 Actual Aggregated Generation per Type B1630 Actual Or Estimated Wind and Solar Power Generation B1720 Amount Of Balancing Reserves Under Contract Service B1730 Prices Of Procured Balancing Reserves Service B1740 Accepted Aggregated Offers B1750 Activated Balancing Energy B1760 Prices Of Activated Balancing Energy B1770 Imbalance Prices B1780 Aggregated Imbalance Volumes B1790 Financial Expenses and Income For Balancing B1810 Cross-Border Balancing Volumes of Exchanged Bids and Offers B1820 Cross-Border Balancing Prices B1830 Cross-border Balancing Energy Activated BOD Bid Offer Level Data CDN Credit Default Notice Data DETSYSPRICES Detailed System Prices DEVINDOD Daily Energy Volume Data DISBSAD Balancing Services Adjustment Action Data FORDAYDEM Forecast Day and Day Ahead Demand Data FREQ Rolling System Frequency FUELHH Half Hourly Outturn Generation by Fuel Type MELIMBALNGC Forecast Day and Day Ahead Margin and Imbalance Data MID Market Index Data MessageDetailRetrieval REMIT Flow - Message List Retrieval MessageListRetrieval REMIT Flow - Message List Retrieval NETBSAD Balancing Service Adjustment Data NONBM Non BM STOR Instructed Volume Data PHYBMDATA Physical Data SYSDEM System Demand SYSWARN System Warnings TEMP Temperature Data WINDFORFUELHH Wind Generation Forecast and Out-turn Data","title":"Data Stream Descriptions"},{"location":"01-utils/","text":"Core Utilities Imports #exports import numpy as np import pandas as pd import re import os import xmltodict from collections import OrderedDict from warnings import warn from IPython.display import JSON from IPython.core.magic import register_cell_magic @register_cell_magic('warn_exceptions') def warn_exceptions(line, cell): try: exec(cell) except Exception as e: warn(str(e)) import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] import requests r = requests.get(f'https://api.bmreports.com/BMRS/B1610/v2?ServiceType=XML&Period=1&APIKey={api_key}&SettlementDate=2020-01-01') r <Response [200]> #exports class RequestError(Exception): def __init__(self, http_code, error_type, description): self.message = f'{http_code} - {error_type}\\n{description}' def __str__(self): return self.message %%warn_exceptions raise RequestError('400', 'Bad Request', 'You did something wrong') <ipython-input-3-72f956aef7ec>:9: UserWarning: 400 - Bad Request You did something wrong warn(str(e)) #exports def check_status(r): r_metadata = xmltodict.parse(r.text)['response']['responseMetadata'] if r_metadata['httpCode'] == '204': warn(f'Data request was succesful but no content was returned') return pd.DataFrame() elif r_metadata['httpCode'] != '200': raise RequestError(r_metadata['httpCode'], r_metadata['errorType'], r_metadata['description']) return None def check_capping(r): r_metadata = xmltodict.parse(r.text)['response']['responseMetadata'] if 'cappingApplied' in r_metadata.keys(): if r_metadata['cappingApplied'] == 'Yes': capping_applied = True else: capping_applied = False else: capping_applied = 'Could not be determined' return capping_applied check_status(r) check_capping(r) False #exports def expand_cols(df, cols_2_expand=[]): if df.size == 0: return df for col in cols_2_expand: new_df_cols = df[col].apply(pd.Series) df[new_df_cols.columns] = new_df_cols df = df.drop(columns=col) s_cols_2_expand = df.iloc[0].apply(type).isin([OrderedDict, dict, list, tuple]) if s_cols_2_expand.sum() > 0: cols_2_expand = s_cols_2_expand[s_cols_2_expand].index df = expand_cols(df, cols_2_expand) return df def parse_xml_response(r): r_dict = xmltodict.parse(r.text) status_check_response = check_status(r) if status_check_response is not None: return status_check_response capping_applied = check_capping(r) data_content = r_dict['response']['responseBody']['responseList']['item'] if isinstance(data_content, list): df = expand_cols(pd.DataFrame(data_content)) elif isinstance(data_content, OrderedDict): df = pd.DataFrame(pd.Series(data_content)).T else: raise ValueError('The returned `data_content` must be one of: `list` or `OrderedDict`') return df df = parse_xml_response(r) df.head() documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity Actual generation Production Realised ELX-EMFIP-AGOG-TS-261 Sequential fixed size block 2020-01-01 Generation 48W00000EWHLW-1U 48W00000EWHLW-1U T_EWHLW-1 ... T_EWHLW-1 EWHLW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 1.87 Actual generation Production Realised ELX-EMFIP-AGOG-TS-202 Sequential fixed size block 2020-01-01 Generation 48W00000TULWW-1U 48W00000TULWW-1U E_TULWW-1 ... E_TULWW-1 TULWW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 15.618 Actual generation Production Realised ELX-EMFIP-AGOG-TS-352 Sequential fixed size block 2020-01-01 Generation 48W000000STAY-4S 48W000000STAY-4S T_STAY-4 ... T_STAY-4 STAY-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 195.8 Actual generation Production Realised ELX-EMFIP-AGOG-TS-359 Sequential fixed size block 2020-01-01 Generation 48W000000TORN-1G 48W000000TORN-1G T_TORN-1 ... T_TORN-1 TORN-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 631.456 Actual generation Production Realised ELX-EMFIP-AGOG-TS-193 Sequential fixed size block 2020-01-01 Generation 48W00000LNMTH-1R 48W00000LNMTH-1R E_LYNE1 ... E_LYNE1 LNMTH-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 130.298 #exports def dt_rng_to_SPs( start_date: pd.Timestamp, end_date: pd.Timestamp, freq: str='30T', tz: str='Europe/London' ): dt_rng = pd.date_range(start_date, end_date, freq=freq, tz=tz) SPs = list((2*(dt_rng.hour + dt_rng.minute/60) + 1).astype(int)) dt_strs = list(dt_rng.strftime('%Y-%m-%d')) df_dates_SPs = pd.DataFrame({'date':dt_strs, 'SP':SPs}, index=dt_rng).astype(str) # Accounting for clock changes clock_change_dt_idxs_dir = pd.Series(dt_rng).apply(lambda dt: dt.utcoffset().total_seconds()).diff().replace(0, np.nan).dropna() for dt_idx, dir_ in clock_change_dt_idxs_dir.items(): dt = dt_rng[dt_idx].date() SPs = (1 + 2*(dt_rng[dt_rng.date==dt] - pd.to_datetime(dt).tz_localize('Europe/London')).total_seconds()/(60*60)).astype(int) df_dates_SPs.loc[df_dates_SPs.index.date==dt, 'SP'] = SPs return df_dates_SPs def parse_local_datetime( df: pd.DataFrame, dt_col: str='settlementDate', SP_col: str='settlementPeriod', freq: str='30T', tz: str='Europe/London' ) -> pd.DataFrame: # preparing start/end dates start_date = pd.to_datetime(df[dt_col].min()) - pd.Timedelta(days=2) end_date = pd.to_datetime(df[dt_col].max()) + pd.Timedelta(days=2) # mapping from date and SP to local datetime df_dates_SPs = dt_rng_to_SPs(start_date, end_date, freq=freq, tz=tz) date_SP_to_ts = {(v[0], str(v[1])): k for k, v in df_dates_SPs.apply(tuple, axis=1).to_dict().items()} df['local_datetime'] = df[[dt_col, SP_col]].apply(tuple, axis=1).map(date_SP_to_ts) # reordering the `local_datetime` column to be first cols = list(df.columns) cols.remove('local_datetime') df = df[['local_datetime'] + cols] return df df = parse_local_datetime(df) df.head() local_datetime documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-261 Sequential fixed size block 2020-01-01 Generation 48W00000EWHLW-1U 48W00000EWHLW-1U ... T_EWHLW-1 EWHLW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 1.87 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-202 Sequential fixed size block 2020-01-01 Generation 48W00000TULWW-1U 48W00000TULWW-1U ... E_TULWW-1 TULWW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 15.618 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-352 Sequential fixed size block 2020-01-01 Generation 48W000000STAY-4S 48W000000STAY-4S ... T_STAY-4 STAY-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 195.8 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-359 Sequential fixed size block 2020-01-01 Generation 48W000000TORN-1G 48W000000TORN-1G ... T_TORN-1 TORN-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 631.456 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-193 Sequential fixed size block 2020-01-01 Generation 48W00000LNMTH-1R 48W00000LNMTH-1R ... E_LYNE1 LNMTH-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 130.298","title":"Utilities"},{"location":"01-utils/#core-utilities","text":"","title":"Core Utilities"},{"location":"01-utils/#imports","text":"#exports import numpy as np import pandas as pd import re import os import xmltodict from collections import OrderedDict from warnings import warn from IPython.display import JSON from IPython.core.magic import register_cell_magic @register_cell_magic('warn_exceptions') def warn_exceptions(line, cell): try: exec(cell) except Exception as e: warn(str(e)) import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] import requests r = requests.get(f'https://api.bmreports.com/BMRS/B1610/v2?ServiceType=XML&Period=1&APIKey={api_key}&SettlementDate=2020-01-01') r <Response [200]> #exports class RequestError(Exception): def __init__(self, http_code, error_type, description): self.message = f'{http_code} - {error_type}\\n{description}' def __str__(self): return self.message %%warn_exceptions raise RequestError('400', 'Bad Request', 'You did something wrong') <ipython-input-3-72f956aef7ec>:9: UserWarning: 400 - Bad Request You did something wrong warn(str(e)) #exports def check_status(r): r_metadata = xmltodict.parse(r.text)['response']['responseMetadata'] if r_metadata['httpCode'] == '204': warn(f'Data request was succesful but no content was returned') return pd.DataFrame() elif r_metadata['httpCode'] != '200': raise RequestError(r_metadata['httpCode'], r_metadata['errorType'], r_metadata['description']) return None def check_capping(r): r_metadata = xmltodict.parse(r.text)['response']['responseMetadata'] if 'cappingApplied' in r_metadata.keys(): if r_metadata['cappingApplied'] == 'Yes': capping_applied = True else: capping_applied = False else: capping_applied = 'Could not be determined' return capping_applied check_status(r) check_capping(r) False #exports def expand_cols(df, cols_2_expand=[]): if df.size == 0: return df for col in cols_2_expand: new_df_cols = df[col].apply(pd.Series) df[new_df_cols.columns] = new_df_cols df = df.drop(columns=col) s_cols_2_expand = df.iloc[0].apply(type).isin([OrderedDict, dict, list, tuple]) if s_cols_2_expand.sum() > 0: cols_2_expand = s_cols_2_expand[s_cols_2_expand].index df = expand_cols(df, cols_2_expand) return df def parse_xml_response(r): r_dict = xmltodict.parse(r.text) status_check_response = check_status(r) if status_check_response is not None: return status_check_response capping_applied = check_capping(r) data_content = r_dict['response']['responseBody']['responseList']['item'] if isinstance(data_content, list): df = expand_cols(pd.DataFrame(data_content)) elif isinstance(data_content, OrderedDict): df = pd.DataFrame(pd.Series(data_content)).T else: raise ValueError('The returned `data_content` must be one of: `list` or `OrderedDict`') return df df = parse_xml_response(r) df.head() documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity Actual generation Production Realised ELX-EMFIP-AGOG-TS-261 Sequential fixed size block 2020-01-01 Generation 48W00000EWHLW-1U 48W00000EWHLW-1U T_EWHLW-1 ... T_EWHLW-1 EWHLW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 1.87 Actual generation Production Realised ELX-EMFIP-AGOG-TS-202 Sequential fixed size block 2020-01-01 Generation 48W00000TULWW-1U 48W00000TULWW-1U E_TULWW-1 ... E_TULWW-1 TULWW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 15.618 Actual generation Production Realised ELX-EMFIP-AGOG-TS-352 Sequential fixed size block 2020-01-01 Generation 48W000000STAY-4S 48W000000STAY-4S T_STAY-4 ... T_STAY-4 STAY-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 195.8 Actual generation Production Realised ELX-EMFIP-AGOG-TS-359 Sequential fixed size block 2020-01-01 Generation 48W000000TORN-1G 48W000000TORN-1G T_TORN-1 ... T_TORN-1 TORN-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 631.456 Actual generation Production Realised ELX-EMFIP-AGOG-TS-193 Sequential fixed size block 2020-01-01 Generation 48W00000LNMTH-1R 48W00000LNMTH-1R E_LYNE1 ... E_LYNE1 LNMTH-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 130.298 #exports def dt_rng_to_SPs( start_date: pd.Timestamp, end_date: pd.Timestamp, freq: str='30T', tz: str='Europe/London' ): dt_rng = pd.date_range(start_date, end_date, freq=freq, tz=tz) SPs = list((2*(dt_rng.hour + dt_rng.minute/60) + 1).astype(int)) dt_strs = list(dt_rng.strftime('%Y-%m-%d')) df_dates_SPs = pd.DataFrame({'date':dt_strs, 'SP':SPs}, index=dt_rng).astype(str) # Accounting for clock changes clock_change_dt_idxs_dir = pd.Series(dt_rng).apply(lambda dt: dt.utcoffset().total_seconds()).diff().replace(0, np.nan).dropna() for dt_idx, dir_ in clock_change_dt_idxs_dir.items(): dt = dt_rng[dt_idx].date() SPs = (1 + 2*(dt_rng[dt_rng.date==dt] - pd.to_datetime(dt).tz_localize('Europe/London')).total_seconds()/(60*60)).astype(int) df_dates_SPs.loc[df_dates_SPs.index.date==dt, 'SP'] = SPs return df_dates_SPs def parse_local_datetime( df: pd.DataFrame, dt_col: str='settlementDate', SP_col: str='settlementPeriod', freq: str='30T', tz: str='Europe/London' ) -> pd.DataFrame: # preparing start/end dates start_date = pd.to_datetime(df[dt_col].min()) - pd.Timedelta(days=2) end_date = pd.to_datetime(df[dt_col].max()) + pd.Timedelta(days=2) # mapping from date and SP to local datetime df_dates_SPs = dt_rng_to_SPs(start_date, end_date, freq=freq, tz=tz) date_SP_to_ts = {(v[0], str(v[1])): k for k, v in df_dates_SPs.apply(tuple, axis=1).to_dict().items()} df['local_datetime'] = df[[dt_col, SP_col]].apply(tuple, axis=1).map(date_SP_to_ts) # reordering the `local_datetime` column to be first cols = list(df.columns) cols.remove('local_datetime') df = df[['local_datetime'] + cols] return df df = parse_local_datetime(df) df.head() local_datetime documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-261 Sequential fixed size block 2020-01-01 Generation 48W00000EWHLW-1U 48W00000EWHLW-1U ... T_EWHLW-1 EWHLW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 1.87 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-202 Sequential fixed size block 2020-01-01 Generation 48W00000TULWW-1U 48W00000TULWW-1U ... E_TULWW-1 TULWW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 15.618 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-352 Sequential fixed size block 2020-01-01 Generation 48W000000STAY-4S 48W000000STAY-4S ... T_STAY-4 STAY-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 195.8 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-359 Sequential fixed size block 2020-01-01 Generation 48W000000TORN-1G 48W000000TORN-1G ... T_TORN-1 TORN-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 631.456 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-193 Sequential fixed size block 2020-01-01 Generation 48W00000LNMTH-1R 48W00000LNMTH-1R ... E_LYNE1 LNMTH-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 130.298","title":"Imports"},{"location":"02-spec-gen/","text":"Open-API Specification Generation Imports #exports import numpy as np import pandas as pd import os import yaml from jinja2 import Template from IPython.display import JSON #exports def init_spec( title='BMRS API', description='API for the Elexon Balancing Mechanism Reporting Service', root_url='https://api.bmreports.com' ): API_spec = dict() API_spec['title'] = title API_spec['description'] = description API_spec['root_url'] = root_url return API_spec API_spec = init_spec() API_spec {'title': 'BMRS API', 'description': 'API for the Elexon Balancing Mechanism Reporting Service', 'root_url': 'https://api.bmreports.com'} #exports def load_endpoints_df(endpoints_fp: str='data/endpoints.csv'): df_endpoints = pd.read_csv(endpoints_fp) date_idxs = (df_endpoints['Sample Data'].str.count('/')==2).replace(np.nan, False) time_idxs = df_endpoints['Sample Data'].str.contains(':').replace(np.nan, False) df_endpoints.loc[date_idxs & ~time_idxs, 'Sample Data'] = pd.to_datetime(df_endpoints.loc[date_idxs & ~time_idxs, 'Sample Data']).dt.strftime('%Y-%m-%d') df_endpoints.loc[date_idxs & time_idxs, 'Sample Data'] = pd.to_datetime(df_endpoints.loc[date_idxs & time_idxs, 'Sample Data']).dt.strftime('%Y-%m-%d %H:%M:%S') df_endpoints['Sample Data'] = df_endpoints['Sample Data'].fillna('') df_endpoints['Field Name'] = df_endpoints['Field Name'].str.replace(' ', '') return df_endpoints df_endpoints = load_endpoints_df('../data/endpoints.csv') df_endpoints.to_csv('../data/endpoints.csv', index=False) df_endpoints.head(3) id name version method direction Field Name Field Type Remarks Mandatory Format Sample Data B1720 Amount Of Balancing Reserves Under Contract Se... 1 get request APIKey String nan Yes nan AP8DA23 B1720 Amount Of Balancing Reserves Under Contract Se... 1 get request SettlementDate String nan Yes YYYY-MM-DD 2021-01-01 B1720 Amount Of Balancing Reserves Under Contract Se... 1 get request Period String nan Yes */1-50 1 #exports def get_endpoint_single_attr(df_endpoint, attribute='version'): attr_val = df_endpoint[attribute].unique() assert len(attr_val)==1, f'Expected only 1 {attribute}, instead found {len(attr_val)}' attr_val = attr_val[0] return attr_val endpoint_id = 'B1720' df_endpoint = df_endpoints.query(f'id==@endpoint_id') get_endpoint_single_attr(df_endpoint, 'version') 1 #exports def init_stream_dict(df_endpoint, endpoint_id): version = get_endpoint_single_attr(df_endpoint, 'version') name = get_endpoint_single_attr(df_endpoint, 'name') stream = dict() stream['endpoint'] = f'/BMRS/{endpoint_id}/v{version}' stream['description'] = name stream['parameters'] = list() return stream stream = init_stream_dict(df_endpoint, endpoint_id) stream {'endpoint': '/BMRS/B1720/v1', 'description': 'Amount Of Balancing Reserves Under Contract Service', 'parameters': []} #exports def add_params_to_stream_dict( df_endpoint: pd.DataFrame, stream: dict, field_type_map: dict={ 'String': 'string', 'Int': 'integer', 'int': 'integer', 'Integer': 'integer', 'Date': 'string' } ): for _, (param_name, param_type, param_sample) in df_endpoint.query('direction==\"request\"')[['Field Name', 'Field Type', 'Sample Data']].iterrows(): parameter = dict() parameter['name'] = param_name parameter['type'] = field_type_map[param_type] if param_type in ['Date']: parameter['format'] = 'date' if param_name == 'APIKey': parameter['format'] = 'password' if param_sample == 'csv/xml': parameter['examples'] = {f'{param_sub_sample}': {'value': param_sub_sample} for param_sub_sample in param_sample.split('/')} else: parameter['example'] = param_sample stream['parameters'] += [parameter] return stream stream = add_params_to_stream_dict(df_endpoint, stream) stream {'endpoint': '/BMRS/B1720/v1', 'description': 'Amount Of Balancing Reserves Under Contract Service', 'parameters': [{'name': 'APIKey', 'type': 'string', 'format': 'password', 'example': 'AP8DA23'}, {'name': 'SettlementDate', 'type': 'string', 'example': '2021-01-01'}, {'name': 'Period', 'type': 'string', 'example': '1'}, {'name': 'ServiceType', 'type': 'string', 'examples': {'csv': {'value': 'csv'}, 'xml': {'value': 'xml'}}}]} #exports def add_streams_to_spec(API_spec, df_endpoints): API_spec['streams'] = list() endpoint_ids = sorted(list(df_endpoints['id'].unique())) for endpoint_id in endpoint_ids: df_endpoint = df_endpoints.query(f'id==@endpoint_id') stream = init_stream_dict(df_endpoint, endpoint_id) stream = add_params_to_stream_dict(df_endpoint, stream) API_spec['streams'] += [stream] return API_spec API_spec = add_streams_to_spec(API_spec, df_endpoints) JSON(API_spec) <IPython.core.display.JSON object> #exports def construct_spec( df_endpoints: pd.DataFrame, title: str='BMRS API', description: str='API for the Elexon Balancing Mechanism Reporting Service', root_url: str='https://api.bmreports.com' ): API_spec = init_spec() API_spec = add_streams_to_spec(API_spec, df_endpoints) return API_spec %%time API_spec = construct_spec(df_endpoints) Wall time: 480 ms #exports def save_spec( API_spec: dict, in_fp: str='../templates/open_api_spec.yaml', out_fp: str='../data/BMRS_API.yaml' ): rendered_schema = Template(open(in_fp).read()).render(API_spec=API_spec) with open(out_fp, 'w') as f: try: f.write(rendered_schema) except e as exc: raise exc save_spec(API_spec) #exports def load_API_yaml(fp='../data/BMRS_API.yaml'): with open(fp, 'r') as stream: try: API_yaml = yaml.safe_load(stream) except yaml.YAMLError as exc: raise exc return API_yaml API_yaml = load_API_yaml(fp='../data/BMRS_API.yaml') JSON(API_yaml) <IPython.core.display.JSON object> # https://app.swaggerhub.com/apis/AyrtonB/default-title/0.1","title":"OpenAPI Spec"},{"location":"02-spec-gen/#open-api-specification-generation","text":"","title":"Open-API Specification Generation"},{"location":"02-spec-gen/#imports","text":"#exports import numpy as np import pandas as pd import os import yaml from jinja2 import Template from IPython.display import JSON #exports def init_spec( title='BMRS API', description='API for the Elexon Balancing Mechanism Reporting Service', root_url='https://api.bmreports.com' ): API_spec = dict() API_spec['title'] = title API_spec['description'] = description API_spec['root_url'] = root_url return API_spec API_spec = init_spec() API_spec {'title': 'BMRS API', 'description': 'API for the Elexon Balancing Mechanism Reporting Service', 'root_url': 'https://api.bmreports.com'} #exports def load_endpoints_df(endpoints_fp: str='data/endpoints.csv'): df_endpoints = pd.read_csv(endpoints_fp) date_idxs = (df_endpoints['Sample Data'].str.count('/')==2).replace(np.nan, False) time_idxs = df_endpoints['Sample Data'].str.contains(':').replace(np.nan, False) df_endpoints.loc[date_idxs & ~time_idxs, 'Sample Data'] = pd.to_datetime(df_endpoints.loc[date_idxs & ~time_idxs, 'Sample Data']).dt.strftime('%Y-%m-%d') df_endpoints.loc[date_idxs & time_idxs, 'Sample Data'] = pd.to_datetime(df_endpoints.loc[date_idxs & time_idxs, 'Sample Data']).dt.strftime('%Y-%m-%d %H:%M:%S') df_endpoints['Sample Data'] = df_endpoints['Sample Data'].fillna('') df_endpoints['Field Name'] = df_endpoints['Field Name'].str.replace(' ', '') return df_endpoints df_endpoints = load_endpoints_df('../data/endpoints.csv') df_endpoints.to_csv('../data/endpoints.csv', index=False) df_endpoints.head(3) id name version method direction Field Name Field Type Remarks Mandatory Format Sample Data B1720 Amount Of Balancing Reserves Under Contract Se... 1 get request APIKey String nan Yes nan AP8DA23 B1720 Amount Of Balancing Reserves Under Contract Se... 1 get request SettlementDate String nan Yes YYYY-MM-DD 2021-01-01 B1720 Amount Of Balancing Reserves Under Contract Se... 1 get request Period String nan Yes */1-50 1 #exports def get_endpoint_single_attr(df_endpoint, attribute='version'): attr_val = df_endpoint[attribute].unique() assert len(attr_val)==1, f'Expected only 1 {attribute}, instead found {len(attr_val)}' attr_val = attr_val[0] return attr_val endpoint_id = 'B1720' df_endpoint = df_endpoints.query(f'id==@endpoint_id') get_endpoint_single_attr(df_endpoint, 'version') 1 #exports def init_stream_dict(df_endpoint, endpoint_id): version = get_endpoint_single_attr(df_endpoint, 'version') name = get_endpoint_single_attr(df_endpoint, 'name') stream = dict() stream['endpoint'] = f'/BMRS/{endpoint_id}/v{version}' stream['description'] = name stream['parameters'] = list() return stream stream = init_stream_dict(df_endpoint, endpoint_id) stream {'endpoint': '/BMRS/B1720/v1', 'description': 'Amount Of Balancing Reserves Under Contract Service', 'parameters': []} #exports def add_params_to_stream_dict( df_endpoint: pd.DataFrame, stream: dict, field_type_map: dict={ 'String': 'string', 'Int': 'integer', 'int': 'integer', 'Integer': 'integer', 'Date': 'string' } ): for _, (param_name, param_type, param_sample) in df_endpoint.query('direction==\"request\"')[['Field Name', 'Field Type', 'Sample Data']].iterrows(): parameter = dict() parameter['name'] = param_name parameter['type'] = field_type_map[param_type] if param_type in ['Date']: parameter['format'] = 'date' if param_name == 'APIKey': parameter['format'] = 'password' if param_sample == 'csv/xml': parameter['examples'] = {f'{param_sub_sample}': {'value': param_sub_sample} for param_sub_sample in param_sample.split('/')} else: parameter['example'] = param_sample stream['parameters'] += [parameter] return stream stream = add_params_to_stream_dict(df_endpoint, stream) stream {'endpoint': '/BMRS/B1720/v1', 'description': 'Amount Of Balancing Reserves Under Contract Service', 'parameters': [{'name': 'APIKey', 'type': 'string', 'format': 'password', 'example': 'AP8DA23'}, {'name': 'SettlementDate', 'type': 'string', 'example': '2021-01-01'}, {'name': 'Period', 'type': 'string', 'example': '1'}, {'name': 'ServiceType', 'type': 'string', 'examples': {'csv': {'value': 'csv'}, 'xml': {'value': 'xml'}}}]} #exports def add_streams_to_spec(API_spec, df_endpoints): API_spec['streams'] = list() endpoint_ids = sorted(list(df_endpoints['id'].unique())) for endpoint_id in endpoint_ids: df_endpoint = df_endpoints.query(f'id==@endpoint_id') stream = init_stream_dict(df_endpoint, endpoint_id) stream = add_params_to_stream_dict(df_endpoint, stream) API_spec['streams'] += [stream] return API_spec API_spec = add_streams_to_spec(API_spec, df_endpoints) JSON(API_spec) <IPython.core.display.JSON object> #exports def construct_spec( df_endpoints: pd.DataFrame, title: str='BMRS API', description: str='API for the Elexon Balancing Mechanism Reporting Service', root_url: str='https://api.bmreports.com' ): API_spec = init_spec() API_spec = add_streams_to_spec(API_spec, df_endpoints) return API_spec %%time API_spec = construct_spec(df_endpoints) Wall time: 480 ms #exports def save_spec( API_spec: dict, in_fp: str='../templates/open_api_spec.yaml', out_fp: str='../data/BMRS_API.yaml' ): rendered_schema = Template(open(in_fp).read()).render(API_spec=API_spec) with open(out_fp, 'w') as f: try: f.write(rendered_schema) except e as exc: raise exc save_spec(API_spec) #exports def load_API_yaml(fp='../data/BMRS_API.yaml'): with open(fp, 'r') as stream: try: API_yaml = yaml.safe_load(stream) except yaml.YAMLError as exc: raise exc return API_yaml API_yaml = load_API_yaml(fp='../data/BMRS_API.yaml') JSON(API_yaml) <IPython.core.display.JSON object> # https://app.swaggerhub.com/apis/AyrtonB/default-title/0.1","title":"Imports"},{"location":"03-raw-methods/","text":"Raw Method Generation Imports #exports import pandas as pd import io import yaml import xmltodict from jinja2 import Template from IPython.display import JSON from ElexonDataPortal.dev import specgen import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API_yaml = specgen.load_API_yaml(fp='../data/BMRS_API.yaml') JSON(API_yaml) <IPython.core.display.JSON object> Raw Method Generation #exports dict_head = lambda dict_, n=5: dict(pd.Series(dict_).head(n)) def clean_path_name( path_name: str='/BMRS/B0610/v1', name_components_to_drop: list=['BMRS', 'v1', 'v2'] ): for name_component_to_drop in name_components_to_drop: path_name = path_name.replace(name_component_to_drop, '') path_name = path_name.strip('/') return path_name path_name = '/BMRS/B0610/v1' clean_path_name(path_name) 'B0610' #exports def get_available_methods( API_yaml: dict, path: str, acceptable_methods: list=['get', 'post', 'put', 'head', 'delete', 'patch', 'options'] ): path_keys = API_yaml['paths'][path].keys() available_methods = list(set(path_keys) - (set(path_keys) - set(acceptable_methods))) return available_methods def extract_parameter_example(parameter): if 'examples' in parameter.keys(): examples = [value['value'] for value in parameter['examples'].values()] example = examples[0] else: example = parameter['example'] return example def construct_parameters(method_details): parameters = list() for parameter in method_details['parameters']: parameter_info = dict() parameter_info['name'] = parameter['name'] parameter_info['type'] = parameter['schema']['type'] parameter_info['example'] = extract_parameter_example(parameter) parameters += [parameter_info] return parameters def construct_path_functions(API_yaml, path): functions = list() root_url = API_yaml['servers'][0]['url'] available_methods = get_available_methods(API_yaml, path) for method in available_methods: function = dict() method_details = API_yaml['paths'][path][method] function['name'] = f'{method}_{clean_path_name(path)}' function['endpoint'] = f'{root_url}{path}' function['description'] = method_details['description'] function['parameters'] = construct_parameters(method_details) functions += [function] return functions def construct_all_functions(API_yaml): functions = list() for path in API_yaml['paths']: functions += construct_path_functions(API_yaml, path) return functions functions = construct_all_functions(API_yaml) functions[0] {'name': 'get_B0610', 'endpoint': 'https://api.bmreports.com/BMRS/B0610/v1', 'description': 'Actual Total Load per Bidding Zone', 'parameters': [{'name': 'APIKey', 'type': 'string', 'example': 'AP8DA23'}, {'name': 'SettlementDate', 'type': 'string', 'example': '2021-01-01'}, {'name': 'Period', 'type': 'string', 'example': '1'}, {'name': 'ServiceType', 'type': 'string', 'example': 'csv'}]} #exports def save_methods( functions: list, in_fp: str='../templates/raw_methods.py', out_fp: str='../ElexonDataPortal/dev/raw.py' ): rendered_schema = Template(open(in_fp).read()).render(functions=functions) with open(out_fp, 'w') as f: try: f.write(rendered_schema) except e as exc: raise exc save_methods(functions) from ElexonDataPortal.dev import raw r = raw.get_B1610( APIKey=api_key, SettlementDate='2020-01-01', Period=1, NGCBMUnitID='*', ServiceType='csv' ) pd.read_csv(io.StringIO(r.text), skiprows=1).head(3) Time Series ID Registered Resource EIC Code BM Unit ID NGC BM Unit ID PSR Type Market Generation Unit EIC Code Market Generation BMU ID Market Generation NGC BM Unit ID Settlement Date SP Quantity (MW) ELX-EMFIP-AGOG-TS-319 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 Generation 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 2020-01-01 1 56.076 ELX-EMFIP-AGOG-TS-320 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 Generation 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 2020-01-01 1 47.456 ELX-EMFIP-AGOG-TS-175 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 Generation 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 2020-01-01 1 3.096","title":"Raw Methods"},{"location":"03-raw-methods/#raw-method-generation","text":"","title":"Raw Method Generation"},{"location":"03-raw-methods/#imports","text":"#exports import pandas as pd import io import yaml import xmltodict from jinja2 import Template from IPython.display import JSON from ElexonDataPortal.dev import specgen import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API_yaml = specgen.load_API_yaml(fp='../data/BMRS_API.yaml') JSON(API_yaml) <IPython.core.display.JSON object>","title":"Imports"},{"location":"03-raw-methods/#raw-method-generation_1","text":"#exports dict_head = lambda dict_, n=5: dict(pd.Series(dict_).head(n)) def clean_path_name( path_name: str='/BMRS/B0610/v1', name_components_to_drop: list=['BMRS', 'v1', 'v2'] ): for name_component_to_drop in name_components_to_drop: path_name = path_name.replace(name_component_to_drop, '') path_name = path_name.strip('/') return path_name path_name = '/BMRS/B0610/v1' clean_path_name(path_name) 'B0610' #exports def get_available_methods( API_yaml: dict, path: str, acceptable_methods: list=['get', 'post', 'put', 'head', 'delete', 'patch', 'options'] ): path_keys = API_yaml['paths'][path].keys() available_methods = list(set(path_keys) - (set(path_keys) - set(acceptable_methods))) return available_methods def extract_parameter_example(parameter): if 'examples' in parameter.keys(): examples = [value['value'] for value in parameter['examples'].values()] example = examples[0] else: example = parameter['example'] return example def construct_parameters(method_details): parameters = list() for parameter in method_details['parameters']: parameter_info = dict() parameter_info['name'] = parameter['name'] parameter_info['type'] = parameter['schema']['type'] parameter_info['example'] = extract_parameter_example(parameter) parameters += [parameter_info] return parameters def construct_path_functions(API_yaml, path): functions = list() root_url = API_yaml['servers'][0]['url'] available_methods = get_available_methods(API_yaml, path) for method in available_methods: function = dict() method_details = API_yaml['paths'][path][method] function['name'] = f'{method}_{clean_path_name(path)}' function['endpoint'] = f'{root_url}{path}' function['description'] = method_details['description'] function['parameters'] = construct_parameters(method_details) functions += [function] return functions def construct_all_functions(API_yaml): functions = list() for path in API_yaml['paths']: functions += construct_path_functions(API_yaml, path) return functions functions = construct_all_functions(API_yaml) functions[0] {'name': 'get_B0610', 'endpoint': 'https://api.bmreports.com/BMRS/B0610/v1', 'description': 'Actual Total Load per Bidding Zone', 'parameters': [{'name': 'APIKey', 'type': 'string', 'example': 'AP8DA23'}, {'name': 'SettlementDate', 'type': 'string', 'example': '2021-01-01'}, {'name': 'Period', 'type': 'string', 'example': '1'}, {'name': 'ServiceType', 'type': 'string', 'example': 'csv'}]} #exports def save_methods( functions: list, in_fp: str='../templates/raw_methods.py', out_fp: str='../ElexonDataPortal/dev/raw.py' ): rendered_schema = Template(open(in_fp).read()).render(functions=functions) with open(out_fp, 'w') as f: try: f.write(rendered_schema) except e as exc: raise exc save_methods(functions) from ElexonDataPortal.dev import raw r = raw.get_B1610( APIKey=api_key, SettlementDate='2020-01-01', Period=1, NGCBMUnitID='*', ServiceType='csv' ) pd.read_csv(io.StringIO(r.text), skiprows=1).head(3) Time Series ID Registered Resource EIC Code BM Unit ID NGC BM Unit ID PSR Type Market Generation Unit EIC Code Market Generation BMU ID Market Generation NGC BM Unit ID Settlement Date SP Quantity (MW) ELX-EMFIP-AGOG-TS-319 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 Generation 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 2020-01-01 1 56.076 ELX-EMFIP-AGOG-TS-320 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 Generation 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 2020-01-01 1 47.456 ELX-EMFIP-AGOG-TS-175 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 Generation 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 2020-01-01 1 3.096","title":"Raw Method Generation"},{"location":"04-client-prep/","text":"Client Preparation Imports #exports from tqdm import tqdm from warnings import warn from functools import reduce from ElexonDataPortal.dev import rawgen, specgen, raw, utils from IPython.display import JSON import pandas as pd import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API_yaml = specgen.load_API_yaml(fp='../data/BMRS_API.yaml') JSON(API_yaml) <IPython.core.display.JSON object> #exports def test_endpoints( default_kwargs: dict ): methods_to_test = [func for func in dir(raw) if 'get_' in func] stream_to_df = dict() for method_to_test in tqdm(methods_to_test): method_func = getattr(raw, method_to_test) func_kwargs = dict(zip(method_func.__code__.co_varnames, method_func.__defaults__)) for kwarg, value in default_kwargs.items(): if kwarg in func_kwargs.keys(): func_kwargs.update({kwarg: value}) r = method_func(**func_kwargs) df = utils.parse_xml_response(r) stream_to_df[method_to_test.split('_')[1]] = df streams_without_content = [] for stream, df in stream_to_df.items(): if df.size == 0: streams_without_content += [stream] return streams_without_content if len(streams_without_content) > 0: warn(f\"The following data streams returned no content data: {', '.join(streams_without_content)}\") return stream_to_df default_kwargs = { 'APIKey': api_key, 'ServiceType': 'xml' } stream_to_df = test_endpoints(default_kwargs) 100% 49/49 [00:35<00:00, 1.39it/s] #exports def construct_method_to_params_dict(API_yaml): method_to_params = reduce(lambda k, v: {**k, **v}, [ { f'{k}_{rawgen.clean_path_name(stream)}': { parameter['name']: rawgen.extract_parameter_example(parameter) for parameter in v['parameters'] } for k, v in method.items() if k in ['get', 'post'] } for stream, method in API_yaml['paths'].items() ]) return method_to_params method_to_params = construct_method_to_params_dict(API_yaml) pd.Series(method_to_params).head(3).to_dict() {'get_B0610': {'APIKey': 'AP8DA23', 'SettlementDate': '2021-01-01', 'Period': '1', 'ServiceType': 'csv'}, 'get_B0620': {'APIKey': 'AP8DA23', 'SettlementDate': '2021-01-01', 'Period': '1', 'ServiceType': 'csv'}, 'get_B0630': {'APIKey': 'AP8DA23', 'Year': '2021', 'Week': '22', 'ServiceType': 'csv'}} flatten_list = lambda list_: [item for sublist in list_ for item in sublist] field_names = sorted(list(set(flatten_list([list(params.keys()) for params in method_to_params.values()])))) field_names ['APIKey', 'ActiveFlag', 'AssetID', 'BMUnitId', 'BMUnitType', 'EndDate', 'EndTime', 'EventEnd', 'EventStart', 'EventType', 'FromClearedDate', 'FromDate', 'FromDateTime', 'FromSettlementDate', 'FuelType', 'LeadPartyName', 'MessageID', 'MessageId', 'MessageType', 'Month', 'NGCBMUnit', 'NGCBMUnitID', 'Name', 'ParticipantId', 'Period', 'ProcessType', 'PublicationFrom', 'PublicationTo', 'SequenceId', 'ServiceType', 'SettlementDate', 'SettlementPeriod', 'StartDate', 'StartTime', 'ToClearedDate', 'ToDate', 'ToDateTime', 'ToSettlementDate', 'UnavailabilityType', 'Week', 'Year', 'isTwoDayWindow'] #exports def construct_request_type_filter( has_start_time: bool, has_end_time: bool, has_start_date: bool, has_end_date: bool, has_date: bool, has_SP: bool, has_year: bool, has_month: bool, has_week: bool ): request_type_filter = { 'year': (has_year + has_month + has_week == 1) and (has_year == 1), 'month': (has_year + has_month == 1) and (has_month == 1), 'week': (has_year + has_week == 1) and (has_week == 1), 'year_and_month': has_year + has_month == 2, 'year_and_week': has_year + has_week == 2, 'SP_and_date': has_SP + has_date == 2, 'date_range': has_start_time + has_end_time + has_start_date + has_end_date == 2, 'date_time_range': has_start_time + has_end_time + has_start_date + has_end_date == 4, 'non_temporal': has_start_time + has_end_time + has_start_date + has_end_date + has_SP + has_date + has_year + has_month == 0, } return request_type_filter def check_request_type_filter( field_names: list, request_type_filter: dict, has_start_time: bool, has_end_time: bool, has_start_date: bool, has_end_date: bool, has_date: bool, has_SP: bool, has_year: bool, has_month: bool, has_week: bool ): \"\"\" Checks the validity of the specified stream parameters The following conditions will raise an error: * has month without a year * has only one of start/end time * has only one of start/end date * has only one settlement period or date * filter does not contain only one request type \"\"\" filter_str = f'\\n\\nFilter:\\n{request_type_filter}\\n\\nField Names:\\n{\", \".join(field_names)}' assert {(False, True): True, (False, False): False, (True, True): False, (True, False): False}[(has_year, has_month)] == False, 'Cannot provide a month without a year' + filter_str assert {(False, True): True, (False, False): False, (True, True): False, (True, False): False}[(has_year, has_week)] == False, 'Cannot provide a week without a year' + filter_str assert has_start_time + has_end_time != 1, 'Only one of start/end time was provided' + filter_str assert has_start_date + has_end_date != 1, 'Only one of start/end date was provided' + filter_str assert (has_SP + has_date != 1) or (has_start_date + has_end_date == 2), 'Only one of date/SP was provided' + filter_str assert sum(request_type_filter.values()) == 1, 'Request type could not be determined\\n\\nFilter' + filter_str return def determine_request_type_from_fields( field_names: list, start_time_cols: list=['StartTime'], end_time_cols: list=['EndTime'], start_date_cols: list=['StartDate', 'FromSettlementDate', 'FromDate'], end_date_cols: list=['EndDate', 'ToSettlementDate', 'ToDate'], date_cols: list=['SettlementDate', 'ImplementationDate', 'DecommissioningDate', 'Date', 'startTimeOfHalfHrPeriod'], SP_cols: list=['SettlementPeriod', 'Period'], year_cols: list=['Year'], month_cols: list=['Month', 'MonthName'], week_cols: list=['Week'] ): has_start_time = bool(set(field_names).intersection(set(start_time_cols))) has_end_time = bool(set(field_names).intersection(set(end_time_cols))) has_start_date = bool(set(field_names).intersection(set(start_date_cols))) has_end_date = bool(set(field_names).intersection(set(end_date_cols))) has_date = bool(set(field_names).intersection(set(date_cols))) has_SP = bool(set(field_names).intersection(set(SP_cols))) has_year = bool(set(field_names).intersection(set(year_cols))) has_month = bool(set(field_names).intersection(set(month_cols))) has_week = bool(set(field_names).intersection(set(week_cols))) request_type_filter = construct_request_type_filter( has_start_time, has_end_time, has_start_date, has_end_date, has_date, has_SP, has_year, has_month, has_week ) check_request_type_filter( field_names, request_type_filter, has_start_time, has_end_time, has_start_date, has_end_date, has_date, has_SP, has_year, has_month, has_week ) request_type = [k for k, v in request_type_filter.items() if v==True][0] return request_type method = 'get_B1610' field_names = list(method_to_params[method].keys()) request_type = determine_request_type_from_fields(field_names) request_type 'SP_and_date' #exports def determine_method_request_types(method_to_params): method_to_request_type = dict() for method in method_to_params.keys(): field_names = list(method_to_params[method].keys()) method_to_request_type[method] = determine_request_type_from_fields(field_names) return method_to_request_type method_to_request_type = determine_method_request_types(method_to_params) pd.Series(method_to_request_type).value_counts() SP_and_date 22 date_time_range 9 non_temporal 8 year 5 year_and_month 3 year_and_week 1 date_range 1 dtype: int64 #exports def construct_method_to_params_map(method_to_params): standardised_params_map = { 'start_time': ['StartTime'], 'end_time': ['EndTime'], 'start_date': ['StartDate', 'FromSettlementDate', 'FromDate'], 'end_date': ['EndDate', 'ToSettlementDate', 'ToDate'], 'date': ['SettlementDate', 'ImplementationDate', 'DecommissioningDate', 'Date', 'startTimeOfHalfHrPeriod'], 'SP': ['SettlementPeriod', 'Period'], 'year': ['Year'], 'month': ['Month', 'MonthName'], 'week': ['Week'] } method_to_params_map = dict() for method, params in method_to_params.items(): method_to_params_map[method] = dict() for param in params.keys(): for standardised_param, bmrs_params in standardised_params_map.items(): if param in bmrs_params: method_to_params_map[method][standardised_param] = param return method_to_params_map method_to_params_map = construct_method_to_params_map(method_to_params) pd.Series(method_to_params_map).head(3).to_dict() {'get_B0610': {'date': 'SettlementDate', 'SP': 'Period'}, 'get_B0620': {'date': 'SettlementDate', 'SP': 'Period'}, 'get_B0630': {'year': 'Year', 'week': 'Week'}} #exports def construct_method_info_dict(API_yaml_fp: str): API_yaml = specgen.load_API_yaml(API_yaml_fp) method_to_params = construct_method_to_params_dict(API_yaml) method_to_request_type = determine_method_request_types(method_to_params) method_to_params_map = construct_method_to_params_map(method_to_params) method_info = dict() for method, params in method_to_params.items(): method_info[method] = dict() method_info[method]['request_type'] = method_to_request_type[method] method_info[method]['kwargs_map'] = method_to_params_map[method] method_info[method]['func_kwargs'] = { ( {v: k for k, v in method_to_params_map[method].items()}[k] if k in method_to_params_map[method].values() else k ): v for k, v in method_to_params[method].items() } return method_info method_info = construct_method_info_dict('../data/BMRS_API.yaml') JSON([method_info]) <IPython.core.display.JSON object>","title":"Endpoint Types"},{"location":"04-client-prep/#client-preparation","text":"","title":"Client Preparation"},{"location":"04-client-prep/#imports","text":"#exports from tqdm import tqdm from warnings import warn from functools import reduce from ElexonDataPortal.dev import rawgen, specgen, raw, utils from IPython.display import JSON import pandas as pd import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API_yaml = specgen.load_API_yaml(fp='../data/BMRS_API.yaml') JSON(API_yaml) <IPython.core.display.JSON object> #exports def test_endpoints( default_kwargs: dict ): methods_to_test = [func for func in dir(raw) if 'get_' in func] stream_to_df = dict() for method_to_test in tqdm(methods_to_test): method_func = getattr(raw, method_to_test) func_kwargs = dict(zip(method_func.__code__.co_varnames, method_func.__defaults__)) for kwarg, value in default_kwargs.items(): if kwarg in func_kwargs.keys(): func_kwargs.update({kwarg: value}) r = method_func(**func_kwargs) df = utils.parse_xml_response(r) stream_to_df[method_to_test.split('_')[1]] = df streams_without_content = [] for stream, df in stream_to_df.items(): if df.size == 0: streams_without_content += [stream] return streams_without_content if len(streams_without_content) > 0: warn(f\"The following data streams returned no content data: {', '.join(streams_without_content)}\") return stream_to_df default_kwargs = { 'APIKey': api_key, 'ServiceType': 'xml' } stream_to_df = test_endpoints(default_kwargs) 100% 49/49 [00:35<00:00, 1.39it/s] #exports def construct_method_to_params_dict(API_yaml): method_to_params = reduce(lambda k, v: {**k, **v}, [ { f'{k}_{rawgen.clean_path_name(stream)}': { parameter['name']: rawgen.extract_parameter_example(parameter) for parameter in v['parameters'] } for k, v in method.items() if k in ['get', 'post'] } for stream, method in API_yaml['paths'].items() ]) return method_to_params method_to_params = construct_method_to_params_dict(API_yaml) pd.Series(method_to_params).head(3).to_dict() {'get_B0610': {'APIKey': 'AP8DA23', 'SettlementDate': '2021-01-01', 'Period': '1', 'ServiceType': 'csv'}, 'get_B0620': {'APIKey': 'AP8DA23', 'SettlementDate': '2021-01-01', 'Period': '1', 'ServiceType': 'csv'}, 'get_B0630': {'APIKey': 'AP8DA23', 'Year': '2021', 'Week': '22', 'ServiceType': 'csv'}} flatten_list = lambda list_: [item for sublist in list_ for item in sublist] field_names = sorted(list(set(flatten_list([list(params.keys()) for params in method_to_params.values()])))) field_names ['APIKey', 'ActiveFlag', 'AssetID', 'BMUnitId', 'BMUnitType', 'EndDate', 'EndTime', 'EventEnd', 'EventStart', 'EventType', 'FromClearedDate', 'FromDate', 'FromDateTime', 'FromSettlementDate', 'FuelType', 'LeadPartyName', 'MessageID', 'MessageId', 'MessageType', 'Month', 'NGCBMUnit', 'NGCBMUnitID', 'Name', 'ParticipantId', 'Period', 'ProcessType', 'PublicationFrom', 'PublicationTo', 'SequenceId', 'ServiceType', 'SettlementDate', 'SettlementPeriod', 'StartDate', 'StartTime', 'ToClearedDate', 'ToDate', 'ToDateTime', 'ToSettlementDate', 'UnavailabilityType', 'Week', 'Year', 'isTwoDayWindow'] #exports def construct_request_type_filter( has_start_time: bool, has_end_time: bool, has_start_date: bool, has_end_date: bool, has_date: bool, has_SP: bool, has_year: bool, has_month: bool, has_week: bool ): request_type_filter = { 'year': (has_year + has_month + has_week == 1) and (has_year == 1), 'month': (has_year + has_month == 1) and (has_month == 1), 'week': (has_year + has_week == 1) and (has_week == 1), 'year_and_month': has_year + has_month == 2, 'year_and_week': has_year + has_week == 2, 'SP_and_date': has_SP + has_date == 2, 'date_range': has_start_time + has_end_time + has_start_date + has_end_date == 2, 'date_time_range': has_start_time + has_end_time + has_start_date + has_end_date == 4, 'non_temporal': has_start_time + has_end_time + has_start_date + has_end_date + has_SP + has_date + has_year + has_month == 0, } return request_type_filter def check_request_type_filter( field_names: list, request_type_filter: dict, has_start_time: bool, has_end_time: bool, has_start_date: bool, has_end_date: bool, has_date: bool, has_SP: bool, has_year: bool, has_month: bool, has_week: bool ): \"\"\" Checks the validity of the specified stream parameters The following conditions will raise an error: * has month without a year * has only one of start/end time * has only one of start/end date * has only one settlement period or date * filter does not contain only one request type \"\"\" filter_str = f'\\n\\nFilter:\\n{request_type_filter}\\n\\nField Names:\\n{\", \".join(field_names)}' assert {(False, True): True, (False, False): False, (True, True): False, (True, False): False}[(has_year, has_month)] == False, 'Cannot provide a month without a year' + filter_str assert {(False, True): True, (False, False): False, (True, True): False, (True, False): False}[(has_year, has_week)] == False, 'Cannot provide a week without a year' + filter_str assert has_start_time + has_end_time != 1, 'Only one of start/end time was provided' + filter_str assert has_start_date + has_end_date != 1, 'Only one of start/end date was provided' + filter_str assert (has_SP + has_date != 1) or (has_start_date + has_end_date == 2), 'Only one of date/SP was provided' + filter_str assert sum(request_type_filter.values()) == 1, 'Request type could not be determined\\n\\nFilter' + filter_str return def determine_request_type_from_fields( field_names: list, start_time_cols: list=['StartTime'], end_time_cols: list=['EndTime'], start_date_cols: list=['StartDate', 'FromSettlementDate', 'FromDate'], end_date_cols: list=['EndDate', 'ToSettlementDate', 'ToDate'], date_cols: list=['SettlementDate', 'ImplementationDate', 'DecommissioningDate', 'Date', 'startTimeOfHalfHrPeriod'], SP_cols: list=['SettlementPeriod', 'Period'], year_cols: list=['Year'], month_cols: list=['Month', 'MonthName'], week_cols: list=['Week'] ): has_start_time = bool(set(field_names).intersection(set(start_time_cols))) has_end_time = bool(set(field_names).intersection(set(end_time_cols))) has_start_date = bool(set(field_names).intersection(set(start_date_cols))) has_end_date = bool(set(field_names).intersection(set(end_date_cols))) has_date = bool(set(field_names).intersection(set(date_cols))) has_SP = bool(set(field_names).intersection(set(SP_cols))) has_year = bool(set(field_names).intersection(set(year_cols))) has_month = bool(set(field_names).intersection(set(month_cols))) has_week = bool(set(field_names).intersection(set(week_cols))) request_type_filter = construct_request_type_filter( has_start_time, has_end_time, has_start_date, has_end_date, has_date, has_SP, has_year, has_month, has_week ) check_request_type_filter( field_names, request_type_filter, has_start_time, has_end_time, has_start_date, has_end_date, has_date, has_SP, has_year, has_month, has_week ) request_type = [k for k, v in request_type_filter.items() if v==True][0] return request_type method = 'get_B1610' field_names = list(method_to_params[method].keys()) request_type = determine_request_type_from_fields(field_names) request_type 'SP_and_date' #exports def determine_method_request_types(method_to_params): method_to_request_type = dict() for method in method_to_params.keys(): field_names = list(method_to_params[method].keys()) method_to_request_type[method] = determine_request_type_from_fields(field_names) return method_to_request_type method_to_request_type = determine_method_request_types(method_to_params) pd.Series(method_to_request_type).value_counts() SP_and_date 22 date_time_range 9 non_temporal 8 year 5 year_and_month 3 year_and_week 1 date_range 1 dtype: int64 #exports def construct_method_to_params_map(method_to_params): standardised_params_map = { 'start_time': ['StartTime'], 'end_time': ['EndTime'], 'start_date': ['StartDate', 'FromSettlementDate', 'FromDate'], 'end_date': ['EndDate', 'ToSettlementDate', 'ToDate'], 'date': ['SettlementDate', 'ImplementationDate', 'DecommissioningDate', 'Date', 'startTimeOfHalfHrPeriod'], 'SP': ['SettlementPeriod', 'Period'], 'year': ['Year'], 'month': ['Month', 'MonthName'], 'week': ['Week'] } method_to_params_map = dict() for method, params in method_to_params.items(): method_to_params_map[method] = dict() for param in params.keys(): for standardised_param, bmrs_params in standardised_params_map.items(): if param in bmrs_params: method_to_params_map[method][standardised_param] = param return method_to_params_map method_to_params_map = construct_method_to_params_map(method_to_params) pd.Series(method_to_params_map).head(3).to_dict() {'get_B0610': {'date': 'SettlementDate', 'SP': 'Period'}, 'get_B0620': {'date': 'SettlementDate', 'SP': 'Period'}, 'get_B0630': {'year': 'Year', 'week': 'Week'}} #exports def construct_method_info_dict(API_yaml_fp: str): API_yaml = specgen.load_API_yaml(API_yaml_fp) method_to_params = construct_method_to_params_dict(API_yaml) method_to_request_type = determine_method_request_types(method_to_params) method_to_params_map = construct_method_to_params_map(method_to_params) method_info = dict() for method, params in method_to_params.items(): method_info[method] = dict() method_info[method]['request_type'] = method_to_request_type[method] method_info[method]['kwargs_map'] = method_to_params_map[method] method_info[method]['func_kwargs'] = { ( {v: k for k, v in method_to_params_map[method].items()}[k] if k in method_to_params_map[method].values() else k ): v for k, v in method_to_params[method].items() } return method_info method_info = construct_method_info_dict('../data/BMRS_API.yaml') JSON([method_info]) <IPython.core.display.JSON object>","title":"Imports"},{"location":"05-orchestrator/","text":"Core API Imports #exports import pandas as pd from tqdm import tqdm from warnings import warn from requests.models import Response from ElexonDataPortal.dev import utils, raw from IPython.display import JSON from ElexonDataPortal.dev import clientprep import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API_yaml_fp = '../data/BMRS_API.yaml' method_info = clientprep.construct_method_info_dict(API_yaml_fp) JSON([method_info]) <IPython.core.display.JSON object> Request Types #exports def if_possible_parse_local_datetime(df): dt_cols_with_period_in_name = ['startTimeOfHalfHrPeriod', 'initialForecastPublishingPeriodCommencingTime', 'latestForecastPublishingPeriodCommencingTime', 'outTurnPublishingPeriodCommencingTime'] dt_cols = [col for col in df.columns if 'date' in col.lower() or col in dt_cols_with_period_in_name] sp_cols = [col for col in df.columns if 'period' in col.lower() and col not in dt_cols_with_period_in_name] if len(dt_cols)==1 and len(sp_cols)==1: df = utils.parse_local_datetime(df, dt_col=dt_cols[0], SP_col=sp_cols[0]) return df def SP_and_date_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) df_dates_SPs = utils.dt_rng_to_SPs(start_date, end_date) date_SP_tuples = list(df_dates_SPs.reset_index().itertuples(index=False, name=None)) for datetime, query_date, SP in tqdm(date_SP_tuples, desc=stream, total=len(date_SP_tuples)): kwargs.update({ kwargs_map['date']: datetime.strftime('%Y-%m-%d'), kwargs_map['SP']: SP, }) missing_kwargs = list(set(func_params) - set(['SP', 'date'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_SP = utils.parse_xml_response(r) df = df.append(df_SP) df = utils.expand_cols(df) df = if_possible_parse_local_datetime(df) return df method_info_mock = { 'get_B1610': { 'request_type': 'SP_and_date', 'kwargs_map': {'date': 'SettlementDate', 'SP': 'Period'}, 'func_kwargs': { 'APIKey': 'AP8DA23', 'date': '2020-01-01', 'SP': '1', 'NGCBMUnitID': '*', 'ServiceType': 'csv' } } } method = 'get_B1610' kwargs = {'NGCBMUnitID': '*'} kwargs_map = method_info_mock[method]['kwargs_map'] func_params = list(method_info_mock[method]['func_kwargs'].keys()) df = SP_and_date_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2020-01-01 01:30', **kwargs ) df.head(3) B1610: 100% 4/4 [00:02<00:00, 1.45it/s] documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity local_datetime Actual generation Production Realised ELX-EMFIP-AGOG-TS-305 Sequential fixed size block 2020-01-01 Generation 48W00000HRSTW-19 48W00000HRSTW-19 T_HRSTW-1 ... HRSTW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 48.346 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-338 Sequential fixed size block 2020-01-01 Generation 48W000000RREW-14 48W000000RREW-14 T_RREW-1 ... RREW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 35.92 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-349 Sequential fixed size block 2020-01-01 Generation 48W000000SIZB-2S 48W000000SIZB-2S T_SIZB-2 ... SIZB-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 599.94 2020-01-01 00:00:00+00:00 #exports def handle_capping( r: Response, df: pd.DataFrame, method: str, kwargs_map: dict, func_params: list, api_key: str, end_date: str, request_type: str, **kwargs ): capping_applied = utils.check_capping(r) assert capping_applied != None, 'No information on whether or not capping limits had been breached could be found in the response metadata' if capping_applied == True: # only subset of date range returned dt_cols_with_period_in_name = ['startTimeOfHalfHrPeriod'] dt_cols = [col for col in df.columns if ('date' in col.lower() or col in dt_cols_with_period_in_name) and ('end' not in col.lower())] if len(dt_cols) == 1: start_date = pd.to_datetime(df[dt_cols[0]]).max().strftime('%Y-%m-%d') if 'start_time' in kwargs.keys(): kwargs['start_time'] = '00:00' if pd.to_datetime(start_date) >= pd.to_datetime(end_date): warnings.warn(f'The `end_date` ({end_date}) was earlier than `start_date` ({start_date})\\nThe `start_date` will be set one day earlier than the `end_date`.') start_date = (pd.to_datetime(end_date) - pd.Timedelta(days=1)).strftime('%Y-%m-%d') warn(f'Response was capped, request is rerunning for missing data from {start_date}') df_rerun = date_range_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date=start_date, end_date=end_date, request_type=request_type, **kwargs ) df = df.append(df_rerun) df = df.drop_duplicates() else: warn(f'Response was capped: a new `start_date` to continue requesting could not be determined automatically, please handle manually for `{method}`') return df def date_range_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, request_type: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) for kwarg in ['start_time', 'end_time']: if kwarg not in kwargs_map.keys(): kwargs_map[kwarg] = kwarg kwargs[kwargs_map['start_date']], kwargs[kwargs_map['start_time']] = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M:%S').split(' ') kwargs[kwargs_map['end_date']], kwargs[kwargs_map['end_time']] = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M:%S').split(' ') if 'SP' in kwargs_map.keys(): kwargs[kwargs_map['SP']] = '*' func_params.remove('SP') func_params += [kwargs_map['SP']] missing_kwargs = list(set(func_params) - set(['start_date', 'end_date', 'start_time', 'end_time'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" if request_type == 'date_range': kwargs.pop(kwargs_map['start_time']) kwargs.pop(kwargs_map['end_time']) r = getattr(raw, method)(**kwargs) utils.check_status(r) df = utils.parse_xml_response(r) df = if_possible_parse_local_datetime(df) # Handling capping df = handle_capping( r, df, method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, end_date=end_date, request_type=request_type, **kwargs ) return df method = 'get_B1540' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) request_type = method_info[method]['request_type'] df = date_range_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2021-01-01', request_type=request_type, **kwargs ) df.head(3) <ipython-input-8-a3b15d7a70c8>:28: UserWarning: Response was capped, request is rerunning for missing data from 2020-06-23 warn(f'Response was capped, request is rerunning for missing data from {start_date}') <ipython-input-8-a3b15d7a70c8>:28: UserWarning: Response was capped, request is rerunning for missing data from 2020-11-03 warn(f'Response was capped, request is rerunning for missing data from {start_date}') <ipython-input-8-a3b15d7a70c8>:28: UserWarning: Response was capped, request is rerunning for missing data from 2020-12-31 warn(f'Response was capped, request is rerunning for missing data from {start_date}') timeSeriesID businessType settlementStartDate settlementStartTime settlementEndDate settlementEndTime quantity prodRegisteredResourcemRID bMUnitID nGCBMUnitID documentType processType activeFlag documentID documentRevNum reasonCode prodRegisteredResourceName prodRegisteredResourcemLocation pSRType reasonDescription MP-NGET-AAPU-TS-00050434 Unplanned outage 2020-06-23 23:00:00 2020-07-07 23:00:00 0 48WSTN0000ABTHBN nan ABTHB Production unavailability Outage information Y 11XINNOGY------2-NGET-AAPU-00050434 597 nan nan nan nan nan nan nan 2020-06-23 23:00:00 2020-07-07 23:00:00 nan nan nan nan Production unavailability Outage information Y 11XINNOGY------2-NGET-AAPU-00050434 597 Failure nan nan nan nan nan nan 2020-06-23 23:00:00 2020-07-07 23:00:00 nan nan nan nan Production unavailability Outage information Y 11XINNOGY------2-NGET-AAPU-00050434 597 Complementary information nan nan nan Other #exports def year_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) start_year = int(pd.to_datetime(start_date).strftime('%Y')) end_year = int(pd.to_datetime(end_date).strftime('%Y')) for year in tqdm(range(start_year, end_year+1), desc=stream): kwargs.update({kwargs_map['year']: year}) missing_kwargs = list(set(func_params) - set(['year'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_year = utils.parse_xml_response(r) df = df.append(df_year) df = if_possible_parse_local_datetime(df) return df method = 'get_B0650' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = year_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2021-01-01 01:30', **kwargs ) df.head(3) B0650: 100% 2/2 [00:01<00:00, 1.06it/s] timeSeriesID businessType year week quantity documentType processType objectAggregation curveType resolution unitOfMeasure monthName activeFlag documentID documentRevNum NGET-EMFIP-YATL-TS-00000762 Maximum available 2020 53 4019 System total load Year ahead Area Sequential fixed size block P7D Mega watt DEC Y NGET-EMFIP-YATL-00000741 1 NGET-EMFIP-YATL-TS-00000781 Minimum possible 2020 53 2181 System total load Year ahead Area Sequential fixed size block P7D Mega watt DEC Y NGET-EMFIP-YATL-00000761 1 NGET-EMFIP-YATL-TS-00000782 Maximum available 2020 53 4100 System total load Year ahead Area Sequential fixed size block P7D Mega watt DEC Y NGET-EMFIP-YATL-00000761 1 #exports def construct_year_month_pairs(start_date, end_date): dt_rng = pd.date_range(start_date, end_date, freq='M') if len(dt_rng) == 0: year_month_pairs = [tuple(pd.to_datetime(start_date).strftime('%Y %b').split(' '))] else: year_month_pairs = [tuple(dt.strftime('%Y %b').split(' ')) for dt in dt_rng] year_month_pairs = [(int(year), week.upper()) for year, week in year_month_pairs] return year_month_pairs def year_and_month_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) year_month_pairs = construct_year_month_pairs(start_date, end_date) for year, month in tqdm(year_month_pairs, desc=stream): kwargs.update({ kwargs_map['year']: year, kwargs_map['month']: month }) missing_kwargs = list(set(func_params) - set(['year', 'month'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_year = utils.parse_xml_response(r) df = df.append(df_year) df = if_possible_parse_local_datetime(df) return df method = 'get_B0640' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = year_and_month_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2020-03-31', **kwargs ) df.head(3) B0640: 100% 3/3 [00:00<00:00, 5.57it/s] timeSeriesID businessType year monthName week quantity documentType processType objectAggregation curveType resolution unitOfMeasure activeFlag documentID documentRevNum NGET-EMFIP-MATL-TS-06201799 Minimum possible 2020 JAN 2020-01-27 20201 System total load Month ahead Area Sequential fixed size block P7D Mega watt Y NGET-EMFIP-MATL-16201797 1 NGET-EMFIP-MATL-TS-06201800 Maximum available 2020 JAN 2020-01-27 53599 System total load Month ahead Area Sequential fixed size block P7D Mega watt Y NGET-EMFIP-MATL-16201797 1 NGET-EMFIP-MATL-TS-06201801 Minimum possible 2020 JAN 2020-01-27 20201 System total load Month ahead Area Sequential fixed size block P7D Mega watt Y NGET-EMFIP-MATL-16201817 1 #exports def clean_year_week(year, week): year = int(year) if week == '00': year = int(year) - 1 week = 52 else: year = int(year) week = int(week.strip('0')) return year, week def construct_year_week_pairs(start_date, end_date): dt_rng = pd.date_range(start_date, end_date, freq='W') if len(dt_rng) == 0: year_week_pairs = [tuple(pd.to_datetime(start_date).strftime('%Y %W').split(' '))] else: year_week_pairs = [tuple(dt.strftime('%Y %W').split(' ')) for dt in dt_rng] year_week_pairs = [clean_year_week(year, week) for year, week in year_week_pairs] return year_week_pairs def year_and_week_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) year_week_pairs = construct_year_week_pairs(start_date, end_date) for year, week in tqdm(year_week_pairs, desc=stream): kwargs.update({ kwargs_map['year']: year, kwargs_map['week']: week }) missing_kwargs = list(set(func_params) - set(['year', 'week'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_year = utils.parse_xml_response(r) df = df.append(df_year) df = if_possible_parse_local_datetime(df) return df method = 'get_B0630' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = year_and_week_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2020-01-31', **kwargs ) df.head(3) B0630: 100% 4/4 [00:00<00:00, 6.22it/s] timeSeriesID businessType settlementDate quantity week documentType processType objectAggregation curveType resolution unitOfMeasure year activeFlag documentID documentRevNum NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-29 22753 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005682 Maximum available 2019-12-29 40156 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-28 23644 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 #exports def non_temporal_request( method: str, api_key: str, **kwargs ): kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) r = getattr(raw, method)(**kwargs) utils.check_status(r) df = utils.parse_xml_response(r) df = if_possible_parse_local_datetime(df) return df Query Orchestrator #exports def query_orchestrator( method: str, api_key: str, request_type: str, kwargs_map: dict=None, func_params: list=None, start_date: str=None, end_date: str=None, **kwargs ): if request_type not in ['non_temporal']: kwargs.update({ 'kwargs_map': kwargs_map, 'func_params': func_params, 'start_date': start_date, 'end_date': end_date, }) if request_type in ['date_range', 'date_time_range']: kwargs.update({ 'request_type': request_type, }) request_type_to_func = { 'SP_and_date': SP_and_date_request, 'date_range': date_range_request, 'date_time_range': date_range_request, 'year': year_request, 'year_and_month': year_and_month_request, 'year_and_week': year_and_week_request, 'non_temporal': non_temporal_request } assert request_type in request_type_to_func.keys(), f\"{request_type} must be one of: {', '.join(request_type_to_func.keys())}\" request_func = request_type_to_func[request_type] df = request_func( method=method, api_key=api_key, **kwargs ) df = df.reset_index(drop=True) return df method = 'get_B0630' start_date = '2020-01-01' end_date = '2020-01-31' request_type = method_info[method]['request_type'] kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = query_orchestrator( method=method, api_key=api_key, request_type=request_type, kwargs_map=kwargs_map, func_params=func_params, start_date=start_date, end_date=end_date ) df.head(3) B0630: 100% 4/4 [00:00<00:00, 6.54it/s] timeSeriesID businessType settlementDate quantity week documentType processType objectAggregation curveType resolution unitOfMeasure year activeFlag documentID documentRevNum NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-29 22753 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005682 Maximum available 2019-12-29 40156 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-28 23644 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1","title":"Orchestrator"},{"location":"05-orchestrator/#core-api","text":"","title":"Core API"},{"location":"05-orchestrator/#imports","text":"#exports import pandas as pd from tqdm import tqdm from warnings import warn from requests.models import Response from ElexonDataPortal.dev import utils, raw from IPython.display import JSON from ElexonDataPortal.dev import clientprep import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API_yaml_fp = '../data/BMRS_API.yaml' method_info = clientprep.construct_method_info_dict(API_yaml_fp) JSON([method_info]) <IPython.core.display.JSON object>","title":"Imports"},{"location":"05-orchestrator/#request-types","text":"#exports def if_possible_parse_local_datetime(df): dt_cols_with_period_in_name = ['startTimeOfHalfHrPeriod', 'initialForecastPublishingPeriodCommencingTime', 'latestForecastPublishingPeriodCommencingTime', 'outTurnPublishingPeriodCommencingTime'] dt_cols = [col for col in df.columns if 'date' in col.lower() or col in dt_cols_with_period_in_name] sp_cols = [col for col in df.columns if 'period' in col.lower() and col not in dt_cols_with_period_in_name] if len(dt_cols)==1 and len(sp_cols)==1: df = utils.parse_local_datetime(df, dt_col=dt_cols[0], SP_col=sp_cols[0]) return df def SP_and_date_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) df_dates_SPs = utils.dt_rng_to_SPs(start_date, end_date) date_SP_tuples = list(df_dates_SPs.reset_index().itertuples(index=False, name=None)) for datetime, query_date, SP in tqdm(date_SP_tuples, desc=stream, total=len(date_SP_tuples)): kwargs.update({ kwargs_map['date']: datetime.strftime('%Y-%m-%d'), kwargs_map['SP']: SP, }) missing_kwargs = list(set(func_params) - set(['SP', 'date'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_SP = utils.parse_xml_response(r) df = df.append(df_SP) df = utils.expand_cols(df) df = if_possible_parse_local_datetime(df) return df method_info_mock = { 'get_B1610': { 'request_type': 'SP_and_date', 'kwargs_map': {'date': 'SettlementDate', 'SP': 'Period'}, 'func_kwargs': { 'APIKey': 'AP8DA23', 'date': '2020-01-01', 'SP': '1', 'NGCBMUnitID': '*', 'ServiceType': 'csv' } } } method = 'get_B1610' kwargs = {'NGCBMUnitID': '*'} kwargs_map = method_info_mock[method]['kwargs_map'] func_params = list(method_info_mock[method]['func_kwargs'].keys()) df = SP_and_date_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2020-01-01 01:30', **kwargs ) df.head(3) B1610: 100% 4/4 [00:02<00:00, 1.45it/s] documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity local_datetime Actual generation Production Realised ELX-EMFIP-AGOG-TS-305 Sequential fixed size block 2020-01-01 Generation 48W00000HRSTW-19 48W00000HRSTW-19 T_HRSTW-1 ... HRSTW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 48.346 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-338 Sequential fixed size block 2020-01-01 Generation 48W000000RREW-14 48W000000RREW-14 T_RREW-1 ... RREW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 35.92 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-349 Sequential fixed size block 2020-01-01 Generation 48W000000SIZB-2S 48W000000SIZB-2S T_SIZB-2 ... SIZB-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 599.94 2020-01-01 00:00:00+00:00 #exports def handle_capping( r: Response, df: pd.DataFrame, method: str, kwargs_map: dict, func_params: list, api_key: str, end_date: str, request_type: str, **kwargs ): capping_applied = utils.check_capping(r) assert capping_applied != None, 'No information on whether or not capping limits had been breached could be found in the response metadata' if capping_applied == True: # only subset of date range returned dt_cols_with_period_in_name = ['startTimeOfHalfHrPeriod'] dt_cols = [col for col in df.columns if ('date' in col.lower() or col in dt_cols_with_period_in_name) and ('end' not in col.lower())] if len(dt_cols) == 1: start_date = pd.to_datetime(df[dt_cols[0]]).max().strftime('%Y-%m-%d') if 'start_time' in kwargs.keys(): kwargs['start_time'] = '00:00' if pd.to_datetime(start_date) >= pd.to_datetime(end_date): warnings.warn(f'The `end_date` ({end_date}) was earlier than `start_date` ({start_date})\\nThe `start_date` will be set one day earlier than the `end_date`.') start_date = (pd.to_datetime(end_date) - pd.Timedelta(days=1)).strftime('%Y-%m-%d') warn(f'Response was capped, request is rerunning for missing data from {start_date}') df_rerun = date_range_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date=start_date, end_date=end_date, request_type=request_type, **kwargs ) df = df.append(df_rerun) df = df.drop_duplicates() else: warn(f'Response was capped: a new `start_date` to continue requesting could not be determined automatically, please handle manually for `{method}`') return df def date_range_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, request_type: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) for kwarg in ['start_time', 'end_time']: if kwarg not in kwargs_map.keys(): kwargs_map[kwarg] = kwarg kwargs[kwargs_map['start_date']], kwargs[kwargs_map['start_time']] = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M:%S').split(' ') kwargs[kwargs_map['end_date']], kwargs[kwargs_map['end_time']] = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M:%S').split(' ') if 'SP' in kwargs_map.keys(): kwargs[kwargs_map['SP']] = '*' func_params.remove('SP') func_params += [kwargs_map['SP']] missing_kwargs = list(set(func_params) - set(['start_date', 'end_date', 'start_time', 'end_time'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" if request_type == 'date_range': kwargs.pop(kwargs_map['start_time']) kwargs.pop(kwargs_map['end_time']) r = getattr(raw, method)(**kwargs) utils.check_status(r) df = utils.parse_xml_response(r) df = if_possible_parse_local_datetime(df) # Handling capping df = handle_capping( r, df, method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, end_date=end_date, request_type=request_type, **kwargs ) return df method = 'get_B1540' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) request_type = method_info[method]['request_type'] df = date_range_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2021-01-01', request_type=request_type, **kwargs ) df.head(3) <ipython-input-8-a3b15d7a70c8>:28: UserWarning: Response was capped, request is rerunning for missing data from 2020-06-23 warn(f'Response was capped, request is rerunning for missing data from {start_date}') <ipython-input-8-a3b15d7a70c8>:28: UserWarning: Response was capped, request is rerunning for missing data from 2020-11-03 warn(f'Response was capped, request is rerunning for missing data from {start_date}') <ipython-input-8-a3b15d7a70c8>:28: UserWarning: Response was capped, request is rerunning for missing data from 2020-12-31 warn(f'Response was capped, request is rerunning for missing data from {start_date}') timeSeriesID businessType settlementStartDate settlementStartTime settlementEndDate settlementEndTime quantity prodRegisteredResourcemRID bMUnitID nGCBMUnitID documentType processType activeFlag documentID documentRevNum reasonCode prodRegisteredResourceName prodRegisteredResourcemLocation pSRType reasonDescription MP-NGET-AAPU-TS-00050434 Unplanned outage 2020-06-23 23:00:00 2020-07-07 23:00:00 0 48WSTN0000ABTHBN nan ABTHB Production unavailability Outage information Y 11XINNOGY------2-NGET-AAPU-00050434 597 nan nan nan nan nan nan nan 2020-06-23 23:00:00 2020-07-07 23:00:00 nan nan nan nan Production unavailability Outage information Y 11XINNOGY------2-NGET-AAPU-00050434 597 Failure nan nan nan nan nan nan 2020-06-23 23:00:00 2020-07-07 23:00:00 nan nan nan nan Production unavailability Outage information Y 11XINNOGY------2-NGET-AAPU-00050434 597 Complementary information nan nan nan Other #exports def year_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) start_year = int(pd.to_datetime(start_date).strftime('%Y')) end_year = int(pd.to_datetime(end_date).strftime('%Y')) for year in tqdm(range(start_year, end_year+1), desc=stream): kwargs.update({kwargs_map['year']: year}) missing_kwargs = list(set(func_params) - set(['year'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_year = utils.parse_xml_response(r) df = df.append(df_year) df = if_possible_parse_local_datetime(df) return df method = 'get_B0650' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = year_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2021-01-01 01:30', **kwargs ) df.head(3) B0650: 100% 2/2 [00:01<00:00, 1.06it/s] timeSeriesID businessType year week quantity documentType processType objectAggregation curveType resolution unitOfMeasure monthName activeFlag documentID documentRevNum NGET-EMFIP-YATL-TS-00000762 Maximum available 2020 53 4019 System total load Year ahead Area Sequential fixed size block P7D Mega watt DEC Y NGET-EMFIP-YATL-00000741 1 NGET-EMFIP-YATL-TS-00000781 Minimum possible 2020 53 2181 System total load Year ahead Area Sequential fixed size block P7D Mega watt DEC Y NGET-EMFIP-YATL-00000761 1 NGET-EMFIP-YATL-TS-00000782 Maximum available 2020 53 4100 System total load Year ahead Area Sequential fixed size block P7D Mega watt DEC Y NGET-EMFIP-YATL-00000761 1 #exports def construct_year_month_pairs(start_date, end_date): dt_rng = pd.date_range(start_date, end_date, freq='M') if len(dt_rng) == 0: year_month_pairs = [tuple(pd.to_datetime(start_date).strftime('%Y %b').split(' '))] else: year_month_pairs = [tuple(dt.strftime('%Y %b').split(' ')) for dt in dt_rng] year_month_pairs = [(int(year), week.upper()) for year, week in year_month_pairs] return year_month_pairs def year_and_month_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) year_month_pairs = construct_year_month_pairs(start_date, end_date) for year, month in tqdm(year_month_pairs, desc=stream): kwargs.update({ kwargs_map['year']: year, kwargs_map['month']: month }) missing_kwargs = list(set(func_params) - set(['year', 'month'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_year = utils.parse_xml_response(r) df = df.append(df_year) df = if_possible_parse_local_datetime(df) return df method = 'get_B0640' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = year_and_month_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2020-03-31', **kwargs ) df.head(3) B0640: 100% 3/3 [00:00<00:00, 5.57it/s] timeSeriesID businessType year monthName week quantity documentType processType objectAggregation curveType resolution unitOfMeasure activeFlag documentID documentRevNum NGET-EMFIP-MATL-TS-06201799 Minimum possible 2020 JAN 2020-01-27 20201 System total load Month ahead Area Sequential fixed size block P7D Mega watt Y NGET-EMFIP-MATL-16201797 1 NGET-EMFIP-MATL-TS-06201800 Maximum available 2020 JAN 2020-01-27 53599 System total load Month ahead Area Sequential fixed size block P7D Mega watt Y NGET-EMFIP-MATL-16201797 1 NGET-EMFIP-MATL-TS-06201801 Minimum possible 2020 JAN 2020-01-27 20201 System total load Month ahead Area Sequential fixed size block P7D Mega watt Y NGET-EMFIP-MATL-16201817 1 #exports def clean_year_week(year, week): year = int(year) if week == '00': year = int(year) - 1 week = 52 else: year = int(year) week = int(week.strip('0')) return year, week def construct_year_week_pairs(start_date, end_date): dt_rng = pd.date_range(start_date, end_date, freq='W') if len(dt_rng) == 0: year_week_pairs = [tuple(pd.to_datetime(start_date).strftime('%Y %W').split(' '))] else: year_week_pairs = [tuple(dt.strftime('%Y %W').split(' ')) for dt in dt_rng] year_week_pairs = [clean_year_week(year, week) for year, week in year_week_pairs] return year_week_pairs def year_and_week_request( method: str, kwargs_map: dict, func_params: list, api_key: str, start_date: str, end_date: str, **kwargs ): assert start_date is not None, '`start_date` must be specified' assert end_date is not None, '`end_date` must be specified' df = pd.DataFrame() stream = '_'.join(method.split('_')[1:]) kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) year_week_pairs = construct_year_week_pairs(start_date, end_date) for year, week in tqdm(year_week_pairs, desc=stream): kwargs.update({ kwargs_map['year']: year, kwargs_map['week']: week }) missing_kwargs = list(set(func_params) - set(['year', 'week'] + list(kwargs.keys()))) assert len(missing_kwargs) == 0, f\"The following kwargs are missing: {', '.join(missing_kwargs)}\" r = getattr(raw, method)(**kwargs) utils.check_status(r) df_year = utils.parse_xml_response(r) df = df.append(df_year) df = if_possible_parse_local_datetime(df) return df method = 'get_B0630' kwargs = {} kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = year_and_week_request( method=method, kwargs_map=kwargs_map, func_params=func_params, api_key=api_key, start_date='2020-01-01', end_date='2020-01-31', **kwargs ) df.head(3) B0630: 100% 4/4 [00:00<00:00, 6.22it/s] timeSeriesID businessType settlementDate quantity week documentType processType objectAggregation curveType resolution unitOfMeasure year activeFlag documentID documentRevNum NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-29 22753 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005682 Maximum available 2019-12-29 40156 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-28 23644 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 #exports def non_temporal_request( method: str, api_key: str, **kwargs ): kwargs.update({ 'APIKey': api_key, 'ServiceType': 'xml' }) r = getattr(raw, method)(**kwargs) utils.check_status(r) df = utils.parse_xml_response(r) df = if_possible_parse_local_datetime(df) return df","title":"Request Types"},{"location":"05-orchestrator/#query-orchestrator","text":"#exports def query_orchestrator( method: str, api_key: str, request_type: str, kwargs_map: dict=None, func_params: list=None, start_date: str=None, end_date: str=None, **kwargs ): if request_type not in ['non_temporal']: kwargs.update({ 'kwargs_map': kwargs_map, 'func_params': func_params, 'start_date': start_date, 'end_date': end_date, }) if request_type in ['date_range', 'date_time_range']: kwargs.update({ 'request_type': request_type, }) request_type_to_func = { 'SP_and_date': SP_and_date_request, 'date_range': date_range_request, 'date_time_range': date_range_request, 'year': year_request, 'year_and_month': year_and_month_request, 'year_and_week': year_and_week_request, 'non_temporal': non_temporal_request } assert request_type in request_type_to_func.keys(), f\"{request_type} must be one of: {', '.join(request_type_to_func.keys())}\" request_func = request_type_to_func[request_type] df = request_func( method=method, api_key=api_key, **kwargs ) df = df.reset_index(drop=True) return df method = 'get_B0630' start_date = '2020-01-01' end_date = '2020-01-31' request_type = method_info[method]['request_type'] kwargs_map = method_info[method]['kwargs_map'] func_params = list(method_info[method]['func_kwargs'].keys()) df = query_orchestrator( method=method, api_key=api_key, request_type=request_type, kwargs_map=kwargs_map, func_params=func_params, start_date=start_date, end_date=end_date ) df.head(3) B0630: 100% 4/4 [00:00<00:00, 6.54it/s] timeSeriesID businessType settlementDate quantity week documentType processType objectAggregation curveType resolution unitOfMeasure year activeFlag documentID documentRevNum NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-29 22753 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005682 Maximum available 2019-12-29 40156 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1 NGET-EMFIP-WATL-TS-00005681 Minimum possible 2019-12-28 23644 52 System total load Week ahead Area Sequential fixed size block P1D Mega watt 2019 Y NGET-EMFIP-WATL-00005681 1","title":"Query Orchestrator"},{"location":"06-client-gen/","text":"Title #exports from jinja2 import Template from ElexonDataPortal.dev import specgen, rawgen, clientprep import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY'] API Client Generation #exports def generate_streams(API_yaml_fp): request_type_to_date_range_example = { 'SP_and_date': ('2020-01-01', '2020-01-01 1:30'), 'date_range': ('2020-01-01', '2020-01-07'), 'date_time_range': ('2020-01-01', '2020-01-07'), 'year': ('2019-01-01', '2021-01-01'), 'year_and_month': ('2020-01-01', '2020-06-01'), 'year_and_week': ('2020-01-01', '2020-06-01'), 'non_temporal': (None, None) } API_yaml = specgen.load_API_yaml(API_yaml_fp) functions = rawgen.construct_all_functions(API_yaml) method_info = clientprep.construct_method_info_dict(API_yaml_fp) streams = list() for function in functions: name = function['name'] function_method_info = method_info[name] stream = dict() stream['name'] = name stream['description'] = function['description'] stream['date_range_example'] = request_type_to_date_range_example[function_method_info['request_type']] stream['extra_kwargs'] = [param for param in function['parameters'] if param['name'] not in list(function_method_info['kwargs_map'].values())+['APIKey', 'ServiceType']] stream['request_type'] = function_method_info['request_type'] stream['kwargs_map'] = function_method_info['kwargs_map'] stream['func_params'] = list(function_method_info['func_kwargs'].keys()) streams += [stream] return streams API_yaml_fp = '../data/BMRS_API.yaml' streams = generate_streams(API_yaml_fp) streams[0] {'name': 'get_B0610', 'description': 'Actual Total Load per Bidding Zone', 'date_range_example': ('2020-01-01', '2020-01-01 1:30'), 'extra_kwargs': [], 'request_type': 'SP_and_date', 'kwargs_map': {'date': 'SettlementDate', 'SP': 'Period'}, 'func_params': ['APIKey', 'date', 'SP', 'ServiceType']} #exports def save_api_client( API_yaml_fp: str, in_fp: str='../templates/api.py', out_fp: str='../ElexonDataPortal/api.py' ): streams = generate_streams(API_yaml_fp) rendered_schema = Template(open(in_fp).read()).render(streams=streams) with open(out_fp, 'w') as f: try: f.write(rendered_schema) except e as exc: raise exc save_api_client(API_yaml_fp) API Client Testing from ElexonDataPortal import api client = api.Client(api_key) df = client.get_B1610() df.head(3) B1610: 100% 4/4 [00:04<00:00, 1.24s/it] documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity local_datetime Actual generation Production Realised ELX-EMFIP-AGOG-TS-180 Sequential fixed size block 2020-01-01 Generation 48W000000FASN-42 48W000000FASN-42 E_FASN-4 ... FASN-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 7.46 NaT Actual generation Production Realised ELX-EMFIP-AGOG-TS-298 Sequential fixed size block 2020-01-01 Generation 48W000000HINB-77 48W000000HINB-77 T_HINB-7 ... HINB-7 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 502.064 NaT Actual generation Production Realised ELX-EMFIP-AGOG-TS-317 Sequential fixed size block 2020-01-01 Generation 48W00000LARYO-4T 48W00000LARYO-4T T_LARYW-4 ... LARYO-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 70.118 NaT client_methods = [method for method in dir(client) if ('__' not in method) and (method not in ['api_key', 'methods'])] method_to_df = dict() for client_method in client_methods: method_to_df[client_method] = getattr(client, client_method)() B0610: 100% 4/4 [00:00<00:00, 6.34it/s] B0620: 100% 4/4 [00:00<00:00, 6.43it/s] B0630: 100% 22/22 [00:05<00:00, 4.14it/s] B0640: 100% 5/5 [00:00<00:00, 5.29it/s] B0650: 100% 3/3 [00:04<00:00, 1.41s/it] c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\utils.py:27: UserWarning: Data request was succesful but no content was returned warn(f'Data request was succesful but no content was returned') B0810: 100% 3/3 [00:00<00:00, 6.40it/s] B0910: 100% 3/3 [00:00<00:00, 5.75it/s] B1320: 100% 4/4 [00:00<00:00, 6.00it/s] B1330: 100% 5/5 [00:00<00:00, 5.62it/s] B1410: 100% 3/3 [00:00<00:00, 5.08it/s] B1420: 100% 3/3 [00:05<00:00, 1.91s/it] B1430: 100% 4/4 [00:01<00:00, 3.89it/s] B1440: 100% 4/4 [00:01<00:00, 3.88it/s] B1610: 100% 4/4 [00:05<00:00, 1.44s/it] B1620: 100% 4/4 [00:00<00:00, 4.33it/s] B1630: 100% 4/4 [00:00<00:00, 5.61it/s] B1720: 100% 4/4 [00:01<00:00, 3.41it/s] B1730: 100% 4/4 [00:00<00:00, 5.47it/s] B1740: 100% 4/4 [00:00<00:00, 5.92it/s] B1750: 100% 4/4 [00:00<00:00, 5.93it/s] B1760: 100% 4/4 [00:00<00:00, 6.24it/s] B1770: 100% 4/4 [00:01<00:00, 2.78it/s] B1780: 100% 4/4 [00:00<00:00, 4.10it/s] B1790: 100% 5/5 [00:00<00:00, 6.13it/s] B1810: 100% 4/4 [00:00<00:00, 6.13it/s] B1820: 100% 4/4 [00:00<00:00, 6.06it/s] B1830: 100% 4/4 [00:00<00:00, 6.28it/s] BOD: 100% 4/4 [00:00<00:00, 5.60it/s] DETSYSPRICES: 100% 4/4 [00:02<00:00, 1.58it/s] DISBSAD: 100% 4/4 [00:00<00:00, 5.19it/s] c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\orchestrator.py:114: UserWarning: Response was capped: a new `start_date` to continue requesting could not be determined automatically, please handle manually for `get_MID` warn(f'Response was capped: a new `start_date` to continue requesting could not be determined automatically, please handle manually for `{method}`') NETBSAD: 100% 4/4 [00:01<00:00, 2.71it/s] PHYBMDATA: 100% 4/4 [00:19<00:00, 4.77s/it] c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\orchestrator.py:98: UserWarning: Response was capped, request is rerunning for missing data from 2020-01-05 warn(f'Response was capped, request is rerunning for missing data from {start_date}') method = 'get_B1440' print(getattr(client, method).__doc__) method_to_df[method].head(3) Generation forecasts for Wind and Solar Parameters: start_date (str) end_date (str) ProcessType (str) timeSeriesID businessType powerSystemResourceType settlementDate processType settlementPeriod quantity documentType curveType resolution activeFlag documentID documentRevNum local_datetime NGET-EMFIP-DGWS-TS-00034592 Solar generation \"Solar\" 2020-01-01 Day Ahead 1 0 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 NaT NGET-EMFIP-DGWS-TS-00034590 Wind generation \"Wind Offshore\" 2020-01-01 Day Ahead 1 2843.18 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 NaT NGET-EMFIP-DGWS-TS-00034591 Wind generation \"Wind Onshore\" 2020-01-01 Day Ahead 1 3024.24 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 NaT","title":"Client Generation"},{"location":"06-client-gen/#title","text":"#exports from jinja2 import Template from ElexonDataPortal.dev import specgen, rawgen, clientprep import os from dotenv import load_dotenv assert load_dotenv('../.env'), 'Environment variables could not be loaded' api_key = os.environ['BMRS_API_KEY']","title":"Title"},{"location":"06-client-gen/#api-client-generation","text":"#exports def generate_streams(API_yaml_fp): request_type_to_date_range_example = { 'SP_and_date': ('2020-01-01', '2020-01-01 1:30'), 'date_range': ('2020-01-01', '2020-01-07'), 'date_time_range': ('2020-01-01', '2020-01-07'), 'year': ('2019-01-01', '2021-01-01'), 'year_and_month': ('2020-01-01', '2020-06-01'), 'year_and_week': ('2020-01-01', '2020-06-01'), 'non_temporal': (None, None) } API_yaml = specgen.load_API_yaml(API_yaml_fp) functions = rawgen.construct_all_functions(API_yaml) method_info = clientprep.construct_method_info_dict(API_yaml_fp) streams = list() for function in functions: name = function['name'] function_method_info = method_info[name] stream = dict() stream['name'] = name stream['description'] = function['description'] stream['date_range_example'] = request_type_to_date_range_example[function_method_info['request_type']] stream['extra_kwargs'] = [param for param in function['parameters'] if param['name'] not in list(function_method_info['kwargs_map'].values())+['APIKey', 'ServiceType']] stream['request_type'] = function_method_info['request_type'] stream['kwargs_map'] = function_method_info['kwargs_map'] stream['func_params'] = list(function_method_info['func_kwargs'].keys()) streams += [stream] return streams API_yaml_fp = '../data/BMRS_API.yaml' streams = generate_streams(API_yaml_fp) streams[0] {'name': 'get_B0610', 'description': 'Actual Total Load per Bidding Zone', 'date_range_example': ('2020-01-01', '2020-01-01 1:30'), 'extra_kwargs': [], 'request_type': 'SP_and_date', 'kwargs_map': {'date': 'SettlementDate', 'SP': 'Period'}, 'func_params': ['APIKey', 'date', 'SP', 'ServiceType']} #exports def save_api_client( API_yaml_fp: str, in_fp: str='../templates/api.py', out_fp: str='../ElexonDataPortal/api.py' ): streams = generate_streams(API_yaml_fp) rendered_schema = Template(open(in_fp).read()).render(streams=streams) with open(out_fp, 'w') as f: try: f.write(rendered_schema) except e as exc: raise exc save_api_client(API_yaml_fp)","title":"API Client Generation"},{"location":"06-client-gen/#api-client-testing","text":"from ElexonDataPortal import api client = api.Client(api_key) df = client.get_B1610() df.head(3) B1610: 100% 4/4 [00:04<00:00, 1.24s/it] documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity local_datetime Actual generation Production Realised ELX-EMFIP-AGOG-TS-180 Sequential fixed size block 2020-01-01 Generation 48W000000FASN-42 48W000000FASN-42 E_FASN-4 ... FASN-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 7.46 NaT Actual generation Production Realised ELX-EMFIP-AGOG-TS-298 Sequential fixed size block 2020-01-01 Generation 48W000000HINB-77 48W000000HINB-77 T_HINB-7 ... HINB-7 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 502.064 NaT Actual generation Production Realised ELX-EMFIP-AGOG-TS-317 Sequential fixed size block 2020-01-01 Generation 48W00000LARYO-4T 48W00000LARYO-4T T_LARYW-4 ... LARYO-4 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 70.118 NaT client_methods = [method for method in dir(client) if ('__' not in method) and (method not in ['api_key', 'methods'])] method_to_df = dict() for client_method in client_methods: method_to_df[client_method] = getattr(client, client_method)() B0610: 100% 4/4 [00:00<00:00, 6.34it/s] B0620: 100% 4/4 [00:00<00:00, 6.43it/s] B0630: 100% 22/22 [00:05<00:00, 4.14it/s] B0640: 100% 5/5 [00:00<00:00, 5.29it/s] B0650: 100% 3/3 [00:04<00:00, 1.41s/it] c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\utils.py:27: UserWarning: Data request was succesful but no content was returned warn(f'Data request was succesful but no content was returned') B0810: 100% 3/3 [00:00<00:00, 6.40it/s] B0910: 100% 3/3 [00:00<00:00, 5.75it/s] B1320: 100% 4/4 [00:00<00:00, 6.00it/s] B1330: 100% 5/5 [00:00<00:00, 5.62it/s] B1410: 100% 3/3 [00:00<00:00, 5.08it/s] B1420: 100% 3/3 [00:05<00:00, 1.91s/it] B1430: 100% 4/4 [00:01<00:00, 3.89it/s] B1440: 100% 4/4 [00:01<00:00, 3.88it/s] B1610: 100% 4/4 [00:05<00:00, 1.44s/it] B1620: 100% 4/4 [00:00<00:00, 4.33it/s] B1630: 100% 4/4 [00:00<00:00, 5.61it/s] B1720: 100% 4/4 [00:01<00:00, 3.41it/s] B1730: 100% 4/4 [00:00<00:00, 5.47it/s] B1740: 100% 4/4 [00:00<00:00, 5.92it/s] B1750: 100% 4/4 [00:00<00:00, 5.93it/s] B1760: 100% 4/4 [00:00<00:00, 6.24it/s] B1770: 100% 4/4 [00:01<00:00, 2.78it/s] B1780: 100% 4/4 [00:00<00:00, 4.10it/s] B1790: 100% 5/5 [00:00<00:00, 6.13it/s] B1810: 100% 4/4 [00:00<00:00, 6.13it/s] B1820: 100% 4/4 [00:00<00:00, 6.06it/s] B1830: 100% 4/4 [00:00<00:00, 6.28it/s] BOD: 100% 4/4 [00:00<00:00, 5.60it/s] DETSYSPRICES: 100% 4/4 [00:02<00:00, 1.58it/s] DISBSAD: 100% 4/4 [00:00<00:00, 5.19it/s] c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\orchestrator.py:114: UserWarning: Response was capped: a new `start_date` to continue requesting could not be determined automatically, please handle manually for `get_MID` warn(f'Response was capped: a new `start_date` to continue requesting could not be determined automatically, please handle manually for `{method}`') NETBSAD: 100% 4/4 [00:01<00:00, 2.71it/s] PHYBMDATA: 100% 4/4 [00:19<00:00, 4.77s/it] c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\orchestrator.py:98: UserWarning: Response was capped, request is rerunning for missing data from 2020-01-05 warn(f'Response was capped, request is rerunning for missing data from {start_date}') method = 'get_B1440' print(getattr(client, method).__doc__) method_to_df[method].head(3) Generation forecasts for Wind and Solar Parameters: start_date (str) end_date (str) ProcessType (str) timeSeriesID businessType powerSystemResourceType settlementDate processType settlementPeriod quantity documentType curveType resolution activeFlag documentID documentRevNum local_datetime NGET-EMFIP-DGWS-TS-00034592 Solar generation \"Solar\" 2020-01-01 Day Ahead 1 0 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 NaT NGET-EMFIP-DGWS-TS-00034590 Wind generation \"Wind Offshore\" 2020-01-01 Day Ahead 1 2843.18 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 NaT NGET-EMFIP-DGWS-TS-00034591 Wind generation \"Wind Onshore\" 2020-01-01 Day Ahead 1 3024.24 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 NaT","title":"API Client Testing"},{"location":"07-cli-rebuild/","text":"Command Line Interface - Library Rebuilding Imports #exports import typer import shutil import pandas as pd from fastcore.foundation import Config from ElexonDataPortal.dev import nbdev, specgen, rawgen, clientgen #exports app = typer.Typer() #exports @app.command() def rebuild_library(): lib_path = str(Config().path('lib_path')) dir_root = f'{lib_path}/..' endpoints_fp = f'{dir_root}/data/endpoints.csv' shutil.rmtree(lib_path) nbdev.prepare_nbdev_module() nbdev.notebook2script() df_endpoints = specgen.load_endpoints_df(endpoints_fp) API_spec = specgen.construct_spec(df_endpoints) specgen.save_spec( API_spec, in_fp=f'{dir_root}/templates/open_api_spec.yaml', out_fp=f'{dir_root}/data/BMRS_API.yaml' ) rawgen.save_methods( functions=rawgen.construct_all_functions(specgen.load_API_yaml(fp=f'{dir_root}/data/BMRS_API.yaml')), in_fp=f'{dir_root}/templates/raw_methods.py', out_fp=f'{dir_root}/ElexonDataPortal/dev/raw.py' ) clientgen.save_api_client( API_yaml_fp=f'{dir_root}/data/BMRS_API.yaml', in_fp=f'{dir_root}/templates/api.py', out_fp=f'{dir_root}/ElexonDataPortal/api.py' ) nbdev.add_extra_code_desc_to_mod() return rebuild_library() Converted 00-documentation.ipynb. Converted 01-utils.ipynb. Converted 02-spec-gen.ipynb. Converted 03-raw-methods.ipynb. Converted 04-client-prep.ipynb. Converted 05-orchestrator.ipynb. Converted 06-client-gen.ipynb. Converted 07-cli-rebuild.ipynb. Converted 08-quick-start.ipynb. Converted 09-map-gen.ipynb. Converted 10-nbdev.ipynb. Converted Example Usage.ipynb. #exports if __name__ == '__main__' and '__file__' in globals(): app()","title":"Rebuilding"},{"location":"07-cli-rebuild/#command-line-interface-library-rebuilding","text":"","title":"Command Line Interface - Library Rebuilding"},{"location":"07-cli-rebuild/#imports","text":"#exports import typer import shutil import pandas as pd from fastcore.foundation import Config from ElexonDataPortal.dev import nbdev, specgen, rawgen, clientgen #exports app = typer.Typer() #exports @app.command() def rebuild_library(): lib_path = str(Config().path('lib_path')) dir_root = f'{lib_path}/..' endpoints_fp = f'{dir_root}/data/endpoints.csv' shutil.rmtree(lib_path) nbdev.prepare_nbdev_module() nbdev.notebook2script() df_endpoints = specgen.load_endpoints_df(endpoints_fp) API_spec = specgen.construct_spec(df_endpoints) specgen.save_spec( API_spec, in_fp=f'{dir_root}/templates/open_api_spec.yaml', out_fp=f'{dir_root}/data/BMRS_API.yaml' ) rawgen.save_methods( functions=rawgen.construct_all_functions(specgen.load_API_yaml(fp=f'{dir_root}/data/BMRS_API.yaml')), in_fp=f'{dir_root}/templates/raw_methods.py', out_fp=f'{dir_root}/ElexonDataPortal/dev/raw.py' ) clientgen.save_api_client( API_yaml_fp=f'{dir_root}/data/BMRS_API.yaml', in_fp=f'{dir_root}/templates/api.py', out_fp=f'{dir_root}/ElexonDataPortal/api.py' ) nbdev.add_extra_code_desc_to_mod() return rebuild_library() Converted 00-documentation.ipynb. Converted 01-utils.ipynb. Converted 02-spec-gen.ipynb. Converted 03-raw-methods.ipynb. Converted 04-client-prep.ipynb. Converted 05-orchestrator.ipynb. Converted 06-client-gen.ipynb. Converted 07-cli-rebuild.ipynb. Converted 08-quick-start.ipynb. Converted 09-map-gen.ipynb. Converted 10-nbdev.ipynb. Converted Example Usage.ipynb. #exports if __name__ == '__main__' and '__file__' in globals(): app()","title":"Imports"},{"location":"08-quick-start/","text":"Quick Start Guide This notebook outlines how to get up and running with the ElexonDataPortal library, a Python client for retrieving data from the Elexon/BMRS API. The core functionality is exposed through the ElexonDataPortal.api module which we'll now import. Client Initialisation We're now ready to initialise the API Client . The key parameter to pass is the api_key , alternatively this can be set as the environment variable BMRS_API_KEY which will then be loaded automatically (as in this example). from ElexonDataPortal import api client = api.Client() # or use `api.Client('your_api_key_here')` client <ElexonDataPortal.api.Client at 0x16e0bf5b8e0> Client Usage The client exposes a methods attribute which provides a quick way to explore what request methods are available alongside a short description of the data they return. import pandas as pd pd.Series(client.methods).head() get_B0610 Actual Total Load per Bidding Zone get_B0620 Day-Ahead Total Load Forecast per Bidding Zone get_B0630 Week-Ahead Total Load Forecast per Bidding Zone get_B0640 Month-Ahead Total Load Forecast Per Bidding Zone get_B0650 Year Ahead Total Load Forecast per Bidding Zone dtype: object Lets look at the docstring for a specific method, in this case the B1610 stream. print(client.get_B1610.__doc__) Actual Generation Output per Generation Unit Parameters: start_date (str) end_date (str) NGCBMUnitID (str) Now we know what to pass in to the get_B1610 method lets call it! Each response (4 in this example) will be automatically cleaned and parsed, then concatenated into a single Pandas DataFrame. start_date = '2020-01-01' end_date = '2020-01-01 1:30' df_B1610 = client.get_B1610(start_date, end_date) df_B1610.head(3) B1610: 100% 4/4 [00:04<00:00, 1.14s/it] local_datetime documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-151 Sequential fixed size block 2020-01-01 Generation 48W00000CAIRW-2E 48W00000CAIRW-2E ... 2__PSTAT001 CAIRW-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 27.926 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-251 Sequential fixed size block 2020-01-01 Generation 48W00000DRAXX-3A 48W00000DRAXX-3A ... T_DRAXX-3 DRAXX-3 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 638.512 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-231 Sequential fixed size block 2020-01-01 Generation 48W00000BLKWW-1L 48W00000BLKWW-1L ... T_BLKWW-1 BLKWW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 10.63 We could see from the docstring that there were more parameters we could specify, lets pass in the NGCBMUnitID and look at the output for a single power plant. In this example we'll look at the power output from 'LARYO-1' , which makes up roughly 1/4 of the London Array wind farm. start_date = '2021-01-01' end_date = '2021-01-02 23:30' NGCBMUnitID = 'LARYO-1' df_LARYO_1 = client.get_B1610(start_date, end_date, NGCBMUnitID) df_LARYO_1.set_index('local_datetime')['quantity'].astype(float).plot() B1610: 100% 96/96 [00:21<00:00, 4.56it/s] <AxesSubplot:xlabel='local_datetime'> Before moving on we'll quickly show what an alternative data stream might return, in this case the B1440 stream which provides 'Generation forecasts for Wind and Solar' . start_date = '2020-01-01' end_date = '2020-01-01 1:30' df_B1440 = client.get_B1440(start_date, end_date) df_B1440.head(3) B1440: 100% 4/4 [00:00<00:00, 6.08it/s] local_datetime timeSeriesID businessType powerSystemResourceType settlementDate processType settlementPeriod quantity documentType curveType resolution activeFlag documentID documentRevNum 2020-01-01 00:00:00+00:00 NGET-EMFIP-DGWS-TS-00034592 Solar generation \"Solar\" 2020-01-01 Day Ahead 1 0 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 2020-01-01 00:00:00+00:00 NGET-EMFIP-DGWS-TS-00034590 Wind generation \"Wind Offshore\" 2020-01-01 Day Ahead 1 2843.18 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 2020-01-01 00:00:00+00:00 NGET-EMFIP-DGWS-TS-00034591 Wind generation \"Wind Onshore\" 2020-01-01 Day Ahead 1 3024.24 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 Date Handling Often a main pain point when working with time-series data is having to handle timezones, this is made more complex when working with data from the power sector as the datetime is expressed through a combination of settlement dates and settlement periods (half-hour trading blocks). ElexonDataPortal has a utility function for determining the settlement dates and periods over a specified date range. from ElexonDataPortal.dev import utils start_date = '2021-01-01' end_date = '2021-01-02' df_dates_SPs = utils.dt_rng_to_SPs(start_date, end_date) df_dates_SPs.head() Unnamed: 0 date SP 2021-01-01 00:00:00+00:00 2021-01-01 1 2021-01-01 00:30:00+00:00 2021-01-01 2 2021-01-01 01:00:00+00:00 2021-01-01 3 2021-01-01 01:30:00+00:00 2021-01-01 4 2021-01-01 02:00:00+00:00 2021-01-01 5 This is then used by the client to determine the correct settlement period and date pairs to request, as well as how to then parse the returned data and create a new column containing the local datetime. In this example we'll look at what happens when the clocks change forwards (there will be 46 SPs on this date). start_date = '2020-03-28 23:00' end_date = '2020-03-29 02:30' df_B1610_clock_change = client.get_B1610(start_date, end_date) df_B1610_clock_change[['local_datetime', 'settlementDate', 'settlementPeriod']].drop_duplicates() B1610: 100% 6/6 [00:05<00:00, 1.12it/s] Unnamed: 0 local_datetime settlementDate settlementPeriod 0 2020-03-28 23:00:00+00:00 2020-03-28 47 159 2020-03-28 23:30:00+00:00 2020-03-28 48 318 2020-03-29 00:00:00+00:00 2020-03-29 1 478 2020-03-29 00:30:00+00:00 2020-03-29 2 639 2020-03-29 02:00:00+01:00 2020-03-29 3 800 2020-03-29 02:30:00+01:00 2020-03-29 4 In this example we'll look at what happens when the clocks change backwards (there will be 50 SPs on this date). start_date = '2020-10-25 00:30' end_date = '2020-10-25 02:00' df_B1610_clock_change = client.get_B1610(start_date, end_date) df_B1610_clock_change[['local_datetime', 'settlementDate', 'settlementPeriod']].drop_duplicates() B1610: 100% 6/6 [00:05<00:00, 1.19it/s] Unnamed: 0 local_datetime settlementDate settlementPeriod 0 2020-10-25 00:30:00+01:00 2020-10-25 2 155 2020-10-25 01:00:00+01:00 2020-10-25 3 305 2020-10-25 01:30:00+01:00 2020-10-25 4 451 2020-10-25 01:00:00+00:00 2020-10-25 5 592 2020-10-25 01:30:00+00:00 2020-10-25 6 729 2020-10-25 02:00:00+00:00 2020-10-25 7 Under the Hood If you've previously written your own code for extracting data from the Elexon/BMRS API then you may be wondering where some of the normal parameters you pass have gone. The differences in the parameters passed are due to 4 core drivers: Standardisation of date range parameter names Removal of the need to specify ServiceType Automatic passing of APIKey after client initialisation Shipped with sensible defaults for all remaining parameters If you wish to make requests using the raw methods these are available through the ElexonDataportal.dev.raw module. We'll quickly make one of these requests, note that in this example we'll specify ServiceType=csv . import io from ElexonDataPortal import dev r = dev.raw.get_B1610( APIKey=api_key, SettlementDate='2020-01-01', Period='1', NGCBMUnitID='*', ServiceType='csv', ) df_B1610_raw_csv = pd.read_csv(io.StringIO(r.content.decode('utf-8')), skiprows=1) df_B1610_raw_csv.head(3) Time Series ID Registered Resource EIC Code BM Unit ID NGC BM Unit ID PSR Type Market Generation Unit EIC Code Market Generation BMU ID Market Generation NGC BM Unit ID Settlement Date SP Quantity (MW) ELX-EMFIP-AGOG-TS-319 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 Generation 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 2020-01-01 1 56.076 ELX-EMFIP-AGOG-TS-320 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 Generation 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 2020-01-01 1 47.456 ELX-EMFIP-AGOG-TS-175 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 Generation 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 2020-01-01 1 3.096 If you wish to use ServiceType=xml you can use the ElexonDataPortal.dev.utils.parse_xml_response function to convert the response into a Pandas DataFrame. r = dev.raw.get_B1610( APIKey=api_key, SettlementDate='2020-01-01', Period='1', NGCBMUnitID='*', ServiceType='xml', ) df_B1610_raw_xml = dev.utils.parse_xml_response(r) df_B1610_raw_xml.head(3) documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity Actual generation Production Realised ELX-EMFIP-AGOG-TS-236 Sequential fixed size block 2020-01-01 Generation 48W00000CLDCW-17 48W00000CLDCW-17 T_CLDCW-1 ... T_CLDCW-1 CLDCW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 96.42 Actual generation Production Realised ELX-EMFIP-AGOG-TS-171 Sequential fixed size block 2020-01-01 Generation 48W00000BRYBW-10 48W00000BRYBW-10 E_BRYBW-1 ... E_BRYBW-1 BRYBW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 46.42 Actual generation Production Realised ELX-EMFIP-AGOG-TS-257 Sequential fixed size block 2020-01-01 Generation 48W000000EECL-15 48W000000EECL-15 T_EECL-1 ... T_EECL-1 EECL-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 232.272 But how do we go from these raw methods to the standardised client that we first saw? The glue that enables this is the ElexonDataPortal.dev.orchestrator.query_orchestrator function which provides a wrapper for the various request types. Crucially, the query_orchestrator provides a mechanism for collating requests over a date range with the different request types broken down into: SP_and_date date_range date_time_range year year_and_month year_and_week non_temporal Let's make a request to the same BMRS stream using the orchestrator. start_date = '2020-01-01' end_date = '2020-01-01' df_B1610_orchestrator = dev.orchestrator.query_orchestrator( api_key=api_key, start_date=start_date, end_date=end_date, method='get_B1610', request_type='SP_and_date', kwargs_map={'date': 'SettlementDate', 'SP': 'Period'}, func_params=['APIKey', 'date', 'SP', 'ServiceType'], ) df_B1610_orchestrator.head(3) B1610: 100% 1/1 [00:01<00:00, 1.11s/it] local_datetime documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-361 Sequential fixed size block 2020-01-01 Generation 48W00000WBURB-27 48W00000WBURB-27 ... T_WBURB-2 WBURB-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 275.38 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-233 Sequential fixed size block 2020-01-01 Generation 48W000000BLLA-2I 48W000000BLLA-2I ... T_BLLA-2 BLLA-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 8.194 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-326 Sequential fixed size block 2020-01-01 Generation 48W000000NANT-1R 48W000000NANT-1R ... T_NANT-1 NANT-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 12.75 It's worth noting that there are some differences in the returned DataFrames. Firstly, when specifying ServiceType=csv the number of columns is much smaller as the data that is nested within the xml representation is simply not included. Secondly, the orchestrator response includes an additional column relative to the xml one even though it itself specifies ServiceType=xml , this is because the orchestrator introduces a new column containing the local datetime as Pandas Timestamps. ( df_B1610_raw_csv.columns.size, df_B1610_raw_xml.columns.size, df_B1610_orchestrator.columns.size ) (11, 21, 22) The response from the ElexonDataPortal.api.Client is the same as that returned by the orchestrator. (df_B1610.columns == df_B1610_orchestrator.columns).mean() == 1 True","title":"Quick-Start"},{"location":"08-quick-start/#quick-start-guide","text":"This notebook outlines how to get up and running with the ElexonDataPortal library, a Python client for retrieving data from the Elexon/BMRS API. The core functionality is exposed through the ElexonDataPortal.api module which we'll now import.","title":"Quick Start Guide"},{"location":"08-quick-start/#client-initialisation","text":"We're now ready to initialise the API Client . The key parameter to pass is the api_key , alternatively this can be set as the environment variable BMRS_API_KEY which will then be loaded automatically (as in this example). from ElexonDataPortal import api client = api.Client() # or use `api.Client('your_api_key_here')` client <ElexonDataPortal.api.Client at 0x16e0bf5b8e0>","title":"Client Initialisation"},{"location":"08-quick-start/#client-usage","text":"The client exposes a methods attribute which provides a quick way to explore what request methods are available alongside a short description of the data they return. import pandas as pd pd.Series(client.methods).head() get_B0610 Actual Total Load per Bidding Zone get_B0620 Day-Ahead Total Load Forecast per Bidding Zone get_B0630 Week-Ahead Total Load Forecast per Bidding Zone get_B0640 Month-Ahead Total Load Forecast Per Bidding Zone get_B0650 Year Ahead Total Load Forecast per Bidding Zone dtype: object Lets look at the docstring for a specific method, in this case the B1610 stream. print(client.get_B1610.__doc__) Actual Generation Output per Generation Unit Parameters: start_date (str) end_date (str) NGCBMUnitID (str) Now we know what to pass in to the get_B1610 method lets call it! Each response (4 in this example) will be automatically cleaned and parsed, then concatenated into a single Pandas DataFrame. start_date = '2020-01-01' end_date = '2020-01-01 1:30' df_B1610 = client.get_B1610(start_date, end_date) df_B1610.head(3) B1610: 100% 4/4 [00:04<00:00, 1.14s/it] local_datetime documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-151 Sequential fixed size block 2020-01-01 Generation 48W00000CAIRW-2E 48W00000CAIRW-2E ... 2__PSTAT001 CAIRW-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 27.926 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-251 Sequential fixed size block 2020-01-01 Generation 48W00000DRAXX-3A 48W00000DRAXX-3A ... T_DRAXX-3 DRAXX-3 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 638.512 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-231 Sequential fixed size block 2020-01-01 Generation 48W00000BLKWW-1L 48W00000BLKWW-1L ... T_BLKWW-1 BLKWW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 10.63 We could see from the docstring that there were more parameters we could specify, lets pass in the NGCBMUnitID and look at the output for a single power plant. In this example we'll look at the power output from 'LARYO-1' , which makes up roughly 1/4 of the London Array wind farm. start_date = '2021-01-01' end_date = '2021-01-02 23:30' NGCBMUnitID = 'LARYO-1' df_LARYO_1 = client.get_B1610(start_date, end_date, NGCBMUnitID) df_LARYO_1.set_index('local_datetime')['quantity'].astype(float).plot() B1610: 100% 96/96 [00:21<00:00, 4.56it/s] <AxesSubplot:xlabel='local_datetime'> Before moving on we'll quickly show what an alternative data stream might return, in this case the B1440 stream which provides 'Generation forecasts for Wind and Solar' . start_date = '2020-01-01' end_date = '2020-01-01 1:30' df_B1440 = client.get_B1440(start_date, end_date) df_B1440.head(3) B1440: 100% 4/4 [00:00<00:00, 6.08it/s] local_datetime timeSeriesID businessType powerSystemResourceType settlementDate processType settlementPeriod quantity documentType curveType resolution activeFlag documentID documentRevNum 2020-01-01 00:00:00+00:00 NGET-EMFIP-DGWS-TS-00034592 Solar generation \"Solar\" 2020-01-01 Day Ahead 1 0 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 2020-01-01 00:00:00+00:00 NGET-EMFIP-DGWS-TS-00034590 Wind generation \"Wind Offshore\" 2020-01-01 Day Ahead 1 2843.18 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1 2020-01-01 00:00:00+00:00 NGET-EMFIP-DGWS-TS-00034591 Wind generation \"Wind Onshore\" 2020-01-01 Day Ahead 1 3024.24 Wind and solar forecast Sequential fixed size block PT30M Y NGET-EMFIP-DGWS-00035923 1","title":"Client Usage"},{"location":"08-quick-start/#date-handling","text":"Often a main pain point when working with time-series data is having to handle timezones, this is made more complex when working with data from the power sector as the datetime is expressed through a combination of settlement dates and settlement periods (half-hour trading blocks). ElexonDataPortal has a utility function for determining the settlement dates and periods over a specified date range. from ElexonDataPortal.dev import utils start_date = '2021-01-01' end_date = '2021-01-02' df_dates_SPs = utils.dt_rng_to_SPs(start_date, end_date) df_dates_SPs.head() Unnamed: 0 date SP 2021-01-01 00:00:00+00:00 2021-01-01 1 2021-01-01 00:30:00+00:00 2021-01-01 2 2021-01-01 01:00:00+00:00 2021-01-01 3 2021-01-01 01:30:00+00:00 2021-01-01 4 2021-01-01 02:00:00+00:00 2021-01-01 5 This is then used by the client to determine the correct settlement period and date pairs to request, as well as how to then parse the returned data and create a new column containing the local datetime. In this example we'll look at what happens when the clocks change forwards (there will be 46 SPs on this date). start_date = '2020-03-28 23:00' end_date = '2020-03-29 02:30' df_B1610_clock_change = client.get_B1610(start_date, end_date) df_B1610_clock_change[['local_datetime', 'settlementDate', 'settlementPeriod']].drop_duplicates() B1610: 100% 6/6 [00:05<00:00, 1.12it/s] Unnamed: 0 local_datetime settlementDate settlementPeriod 0 2020-03-28 23:00:00+00:00 2020-03-28 47 159 2020-03-28 23:30:00+00:00 2020-03-28 48 318 2020-03-29 00:00:00+00:00 2020-03-29 1 478 2020-03-29 00:30:00+00:00 2020-03-29 2 639 2020-03-29 02:00:00+01:00 2020-03-29 3 800 2020-03-29 02:30:00+01:00 2020-03-29 4 In this example we'll look at what happens when the clocks change backwards (there will be 50 SPs on this date). start_date = '2020-10-25 00:30' end_date = '2020-10-25 02:00' df_B1610_clock_change = client.get_B1610(start_date, end_date) df_B1610_clock_change[['local_datetime', 'settlementDate', 'settlementPeriod']].drop_duplicates() B1610: 100% 6/6 [00:05<00:00, 1.19it/s] Unnamed: 0 local_datetime settlementDate settlementPeriod 0 2020-10-25 00:30:00+01:00 2020-10-25 2 155 2020-10-25 01:00:00+01:00 2020-10-25 3 305 2020-10-25 01:30:00+01:00 2020-10-25 4 451 2020-10-25 01:00:00+00:00 2020-10-25 5 592 2020-10-25 01:30:00+00:00 2020-10-25 6 729 2020-10-25 02:00:00+00:00 2020-10-25 7","title":"Date Handling"},{"location":"08-quick-start/#under-the-hood","text":"If you've previously written your own code for extracting data from the Elexon/BMRS API then you may be wondering where some of the normal parameters you pass have gone. The differences in the parameters passed are due to 4 core drivers: Standardisation of date range parameter names Removal of the need to specify ServiceType Automatic passing of APIKey after client initialisation Shipped with sensible defaults for all remaining parameters If you wish to make requests using the raw methods these are available through the ElexonDataportal.dev.raw module. We'll quickly make one of these requests, note that in this example we'll specify ServiceType=csv . import io from ElexonDataPortal import dev r = dev.raw.get_B1610( APIKey=api_key, SettlementDate='2020-01-01', Period='1', NGCBMUnitID='*', ServiceType='csv', ) df_B1610_raw_csv = pd.read_csv(io.StringIO(r.content.decode('utf-8')), skiprows=1) df_B1610_raw_csv.head(3) Time Series ID Registered Resource EIC Code BM Unit ID NGC BM Unit ID PSR Type Market Generation Unit EIC Code Market Generation BMU ID Market Generation NGC BM Unit ID Settlement Date SP Quantity (MW) ELX-EMFIP-AGOG-TS-319 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 Generation 48W00000LNCSO-1R T_LNCSW-1 LNCSO-1 2020-01-01 1 56.076 ELX-EMFIP-AGOG-TS-320 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 Generation 48W00000LNCSO-2P T_LNCSW-2 LNCSO-2 2020-01-01 1 47.456 ELX-EMFIP-AGOG-TS-175 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 Generation 48W00000CLDRW-16 E_CLDRW-1 CLDRW-1 2020-01-01 1 3.096 If you wish to use ServiceType=xml you can use the ElexonDataPortal.dev.utils.parse_xml_response function to convert the response into a Pandas DataFrame. r = dev.raw.get_B1610( APIKey=api_key, SettlementDate='2020-01-01', Period='1', NGCBMUnitID='*', ServiceType='xml', ) df_B1610_raw_xml = dev.utils.parse_xml_response(r) df_B1610_raw_xml.head(3) documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode marketGenerationBMUId ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity Actual generation Production Realised ELX-EMFIP-AGOG-TS-236 Sequential fixed size block 2020-01-01 Generation 48W00000CLDCW-17 48W00000CLDCW-17 T_CLDCW-1 ... T_CLDCW-1 CLDCW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 96.42 Actual generation Production Realised ELX-EMFIP-AGOG-TS-171 Sequential fixed size block 2020-01-01 Generation 48W00000BRYBW-10 48W00000BRYBW-10 E_BRYBW-1 ... E_BRYBW-1 BRYBW-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 46.42 Actual generation Production Realised ELX-EMFIP-AGOG-TS-257 Sequential fixed size block 2020-01-01 Generation 48W000000EECL-15 48W000000EECL-15 T_EECL-1 ... T_EECL-1 EECL-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 232.272 But how do we go from these raw methods to the standardised client that we first saw? The glue that enables this is the ElexonDataPortal.dev.orchestrator.query_orchestrator function which provides a wrapper for the various request types. Crucially, the query_orchestrator provides a mechanism for collating requests over a date range with the different request types broken down into: SP_and_date date_range date_time_range year year_and_month year_and_week non_temporal Let's make a request to the same BMRS stream using the orchestrator. start_date = '2020-01-01' end_date = '2020-01-01' df_B1610_orchestrator = dev.orchestrator.query_orchestrator( api_key=api_key, start_date=start_date, end_date=end_date, method='get_B1610', request_type='SP_and_date', kwargs_map={'date': 'SettlementDate', 'SP': 'Period'}, func_params=['APIKey', 'date', 'SP', 'ServiceType'], ) df_B1610_orchestrator.head(3) B1610: 100% 1/1 [00:01<00:00, 1.11s/it] local_datetime documentType businessType processType timeSeriesID curveType settlementDate powerSystemResourceType registeredResourceEICCode marketGenerationUnitEICCode ... bMUnitID nGCBMUnitID activeFlag documentID documentRevNum resolution start end settlementPeriod quantity 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-361 Sequential fixed size block 2020-01-01 Generation 48W00000WBURB-27 48W00000WBURB-27 ... T_WBURB-2 WBURB-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 275.38 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-233 Sequential fixed size block 2020-01-01 Generation 48W000000BLLA-2I 48W000000BLLA-2I ... T_BLLA-2 BLLA-2 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 8.194 2020-01-01 00:00:00+00:00 Actual generation Production Realised ELX-EMFIP-AGOG-TS-326 Sequential fixed size block 2020-01-01 Generation 48W000000NANT-1R 48W000000NANT-1R ... T_NANT-1 NANT-1 Y ELX-EMFIP-AGOG-22495386 1 PT30M 2020-01-01 2020-01-01 1 12.75 It's worth noting that there are some differences in the returned DataFrames. Firstly, when specifying ServiceType=csv the number of columns is much smaller as the data that is nested within the xml representation is simply not included. Secondly, the orchestrator response includes an additional column relative to the xml one even though it itself specifies ServiceType=xml , this is because the orchestrator introduces a new column containing the local datetime as Pandas Timestamps. ( df_B1610_raw_csv.columns.size, df_B1610_raw_xml.columns.size, df_B1610_orchestrator.columns.size ) (11, 21, 22) The response from the ElexonDataPortal.api.Client is the same as that returned by the orchestrator. (df_B1610.columns == df_B1610_orchestrator.columns).mean() == 1 True","title":"Under the Hood"},{"location":"09-map-gen/","text":"Map Generation This notebook shows the development of the code required to generate the live map of energy generation by power plant. Imports #exports import json import numpy as np import pandas as pd import geopandas as gpd import os import typer from tqdm import tqdm from jinja2 import Template from ElexonDataPortal.dev import utils, raw from IPython.display import JSON import matplotlib.pyplot as plt Network Route Map The raw network route map data can be retrieved from here . We want to extract the relevant data, group the different cables/lines according to capacity, then save the information as a geojson. #exports def clean_route_gdf(gdf): s_LV_routes = gdf['OPERATING_'].astype(float).fillna(0)<=132 gdf.loc[s_LV_routes, 'OPERATING_'] = '<=132' gdf.loc[~s_LV_routes, 'OPERATING_'] = gdf.loc[~s_LV_routes, 'OPERATING_'].astype(int).astype(str) return gdf def load_route_gdf(data_dir='data'): gdf_OHL = gpd.read_file(f'{data_dir}/transmission-system/OHL.shp').to_crs('EPSG:4326') gdf_cables = gpd.read_file(f'{data_dir}/transmission-system/Cable.shp').to_crs('EPSG:4326') gdf_OHL = clean_route_gdf(gdf_OHL) gdf_cables = clean_route_gdf(gdf_cables) gdf_route = gdf_OHL.append(gdf_cables)[['OPERATING_', 'geometry']].rename(columns={'OPERATING_': 'kV'}) gdf_route.to_file(f'{data_dir}/network_routes.json', driver='GeoJSON') return gdf_route gdf_route = load_route_gdf('../data') gdf_route.head(3) kV geometry 400 LINESTRING Z (-2.02839 51.85078 0.00000, -2.02... 400 LINESTRING Z (-2.02754 51.84931 0.00000, -2.02... 400 LINESTRING Z (-3.12672 51.20292 0.00000, -3.12... We'll quickly visualise the network route and count how many of the different capacity types we have. gdf_route.plot(column='kV', legend=True, legend_kwds={'frameon': False}) gdf_route['kV'].value_counts() 275 1858 400 1232 <=132 1087 Name: kV, dtype: int64 Power Plant Output Data Retrieval We'll begin by creating a function to retrieve and clean Physical Notification (PN) data for transmission level plants in Great Britain. #exports def construct_df_PN_pivot_dt_rng(df_PN): no_seconds = (((df_PN['timeFrom'].str.split(':').str[-1]=='00').mean()==1) & ((df_PN['timeTo'].str.split(':').str[-1]=='00').mean()==1)) if no_seconds == True: dt_rng = pd.date_range(df_PN['timeFrom'].min(), df_PN['timeTo'].max(), freq='min', tz='Europe/London') else: dt_rng = pd.date_range(df_PN['timeFrom'].min(), df_PN['timeTo'].max(), freq='s', tz='Europe/London') return dt_rng def construct_PN_pivot_df(df_PN, resample=None): bmu_ids = sorted(list(df_PN['bmUnitID'].unique())) df_PN_pivot = pd.DataFrame(index=construct_df_PN_pivot_dt_rng(df_PN), columns=bmu_ids, dtype=float) for bmu_id in tqdm(bmu_ids): for idx, row in df_PN.query('bmUnitID==@bmu_id').iterrows(): df_PN_pivot.loc[pd.to_datetime(row['timeFrom']).tz_localize('Europe/London'), bmu_id] = float(row['pnLevelFrom']) df_PN_pivot.loc[pd.to_datetime(row['timeTo']).tz_localize('Europe/London'), bmu_id] = float(row['pnLevelTo']) df_PN_pivot[bmu_id] = df_PN_pivot[bmu_id].interpolate() if resample is not None: df_PN_pivot = df_PN_pivot.resample(resample).mean() return df_PN_pivot def get_PHYBM_df(api_key, start_date=None, end_date=None, record_type='PN', resample='30T'): if start_date is None and end_date is None: start_date = pd.Timestamp.now().round('30min') - pd.Timedelta(minutes=60*1) end_date = pd.Timestamp.now().round('30min') + pd.Timedelta(minutes=180) elif start_date is not None and end_date is not None: pass else: raise ValueError('Only one of `start_date` and `end_date` was specified') df = pd.DataFrame() df_local_datetime_to_date_SP = utils.dt_rng_to_SPs(start_date, end_date) for idx, (date, SP) in tqdm(df_local_datetime_to_date_SP.iterrows(), total=df_local_datetime_to_date_SP.shape[0]): df_SP = utils.parse_xml_response(raw.get_PHYBMDATA(api_key, SettlementDate=date, SettlementPeriod=SP, ServiceType='xml')) df = df.append(df_SP) df = (df .query(f'recordType==\"{record_type}\"') .dropna(how='all', axis=1) ) if resample is not None: df = df.pipe(construct_PN_pivot_df, resample=resample) df.index.name = 'local_datetime' return df df_PN = get_PHYBM_df(api_key) df_PN.index.min(), df_PN.index.max() 44%\u00e2\u2013\u017d | 4/9 [00:13<00:15, 3.13s/it]c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\utils.py:29: UserWarning: Data request was succesful but no content was returned warn(f'Data request was succesful but no content was returned') 100% 9/9 [00:14<00:00, 1.58s/it] 100% 1370/1370 [00:10<00:00, 133.33it/s] (Timestamp('2021-06-25 15:00:00+0100', tz='Europe/London', freq='30T'), Timestamp('2021-06-25 17:00:00+0100', tz='Europe/London', freq='30T')) We'll now inspect the data from the closest half-hour settlement period that is available. nearest_half_hour = (pd.Timestamp.now(tz='Europe/London')+pd.Timedelta(minutes=15)).round('30min') most_recent_available_dt = max(df_PN.index[df_PN.index<=nearest_half_hour]) s_PN = df_PN.loc[most_recent_available_dt] s_PN.replace(0, np.nan).dropna().head() 2__ABGAS000 -378.0 2__AEELC000 -174.0 2__AEMEB000 -200.0 2__AENRD000 -145.0 2__AEOND000 -177.0 Name: 2021-06-25 17:00:00+01:00, dtype: float64 We want to be able to show not just current but historical data, however the time it takes to download the PN is quite long. To get around this issue we'll save data in monthly batches and only request data that is not currently contained in the existing csv files. #exports get_files = lambda data_dir: [f for f in os.listdir(data_dir) if '.csv' in f] def download_latest_PHYBM_data(api_key=None, data_dir='data/PN', record_type='PN'): if api_key is None: if 'BMRS_API_KEY' in os.environ.keys(): api_key = os.environ['BMRS_API_KEY'] else: raise ValueError('`api_key` must be passed or set as the environment variable `BMRS_API_KEY`') files = get_files(data_dir) years_months_downloaded = [f.split('.')[0] for f in files] current_ts = pd.Timestamp.now(tz='Europe/London') current_year_month = current_ts.strftime('%Y-%m') if current_year_month not in years_months_downloaded: start_date, end_date = f'{current_year_month}-01 00:00', current_ts.strftime('%Y-%m-%d %H:%M') df = get_PHYBM_df(api_key, start_date, end_date, record_type=record_type) df.to_csv(f'{data_dir}/{current_year_month}.csv') else: df = pd.read_csv(f'{data_dir}/{current_year_month}.csv') df = df.set_index('local_datetime') df.index = pd.to_datetime(df.index, utc=True).tz_convert('Europe/London') dt_rng = pd.date_range(df.index.max(), current_ts, freq='30T', tz='Europe/London') if dt_rng.size > 1: start_date = dt_rng[0] - pd.Timedelta(minutes=30) end_date = dt_rng[-1] + pd.Timedelta(minutes=60) try: df_latest = get_PHYBM_df(api_key, start_date, end_date, record_type=record_type) df_trimmed = df.drop(list(set(df_latest.index) - (set(df_latest.index) - set(df.index)))) df_combined = df_trimmed.append(df_latest).sort_index() df_combined.to_csv(f'{data_dir}/{current_year_month}.csv') except: warn(f'Could not retrieve any new data between {start_date} and {end_date}') download_latest_PHYBM_data(api_key, data_dir='../data/PN') 100% 5/5 [00:18<00:00, 3.78s/it] 100% 1370/1370 [00:11<00:00, 118.20it/s] We'll also create a helper function for loading in the latest data, when there is less than a week's worth of data in the latest month we'll append the dataframe to last months data as well. #exports def load_most_recent_PN_data(data_dir='data/PN', latest_year_month_file=None, df_PN_old=None): if latest_year_month_file is None: PN_files = sorted(get_files(data_dir)) latest_year_month_file = PN_files[-1] latest_fp = f'{data_dir}/{latest_year_month_file}' df_PN = pd.read_csv(latest_fp) df_PN['local_datetime'] = pd.to_datetime(df_PN['local_datetime'], utc=True) df_PN = df_PN.set_index('local_datetime') df_PN.index = df_PN.index.tz_convert('Europe/London') if df_PN_old is not None: assert latest_year_month_file is not None, 'Should not be appending to the main dataframe if `latest_year_month_file` was not specified' df_PN = df_PN_old.append(df_PN) if df_PN.shape[0] < (48*7): # want to have at least a week's worth of data assert 'PN_files' in locals(), 'The two most recent files combined have less than one week\\'s worth of data' df_PN = load_most_recent_PN_data(data_dir, latest_year_month_file=PN_files[-2], df_PN_old=df_PN) return df_PN df_PN = load_most_recent_PN_data(data_dir='../data/PN') df_PN.tail() local_datetime 2__AANGE001 2__AANGE002 2__ABGAS000 2__ACNDL001 2__AEDFE000 2__AEDIR000 2__AEELC000 2__AEENG000 2__AEMEB000 2__AENRD000 ... V__KGAZP002 V__LCEND001 V__LFLEX001 V__MADEL001 V__MGBLO001 V__NFLEX001 V__PFLEX001 I_I2D-MQBN1 I_I2G-MQBN1 I_IFG-MQBN1 2021-06-25 15:00:00+01:00 0 0 -299.917 0 0 0 -154.833 0 -177.8 -171.617 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 15:30:00+01:00 0 0 -335.4 0 0 0 -161.933 0 -188.35 -162.2 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 16:00:00+01:00 0 0 -365.6 0 0 0 -168.833 0 -196.383 -150.683 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 16:30:00+01:00 0 0 -378 0 0 0 -174 0 -200 -145 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 17:00:00+01:00 0 0 -378 0 0 0 -174 0 -200 -145 ... 0 0 0 0 0 0 0 0 0 0 Location & Fuel Type Data Collation Now we're redy to start building the map, for this we'll need information on both the location and fuel-type (which will be represented by the colour of the plant). We can get this data from the power station dictionary project. df_powerdict = pd.read_csv('https://raw.githubusercontent.com/OSUKED/Power-Station-Dictionary/main/data/output/power_stations.csv') df_powerdict.head(3) osuked_id esail_id gppd_idnr name sett_bmu_id longitude latitude fuel_type capacity_mw 10000 MARK nan Rothes Bio-Plant CHP E_MARK-1, E_MARK-2 -3.60352 57.4804 biomass nan 10001 DIDC nan Didcot A (G) T_DIDC1, T_DIDC2, T_DIDC4, T_DIDC3 -1.26757 51.6236 coal nan 10002 ABTH GBR1000374 Aberthaw B T_ABTH7, T_ABTH8, T_ABTH9 -3.40487 51.3873 coal 1586 We'll create some helper dictionaries for mapping from the osuked id to the data we're interested in #exports def construct_osuked_id_mappings(df_powerdict): osuked_id_mappings = dict() osuked_id_mappings['bmu_ids'] = (df_powerdict .set_index('osuked_id') ['sett_bmu_id'] .str.split(', ') .dropna() .to_dict() ) osuked_id_mappings['capacity_mw'] = (df_powerdict .set_index('osuked_id') ['capacity_mw'] .astype(str) .str.replace('.0', '', regex=False) .to_dict() ) osuked_id_mappings['fuel_type'] = (df_powerdict .set_index('osuked_id') ['fuel_type'] .dropna() .to_dict() ) osuked_id_mappings['name'] = (df_powerdict .set_index('osuked_id') ['name'] .dropna() .to_dict() ) osuked_id_mappings['lat_lon'] = (df_powerdict .set_index('osuked_id') [['latitude', 'longitude']] .dropna() .apply(dict, axis=1) .to_dict() ) return osuked_id_mappings osuked_id_mappings = construct_osuked_id_mappings(df_powerdict) osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_fuel_type, osuked_id_to_name, osuked_id_to_lat_lon = osuked_id_mappings.values() pd.Series(osuked_id_to_bmu_ids).head().to_dict() {10000: ['E_MARK-1', 'E_MARK-2'], 10001: ['T_DIDC1', 'T_DIDC2', 'T_DIDC4', 'T_DIDC3'], 10002: ['T_ABTH7', 'T_ABTH8', 'T_ABTH9'], 10003: ['T_COTPS-1', 'T_COTPS-2', 'T_COTPS-3', 'T_COTPS-4'], 10004: ['T_DRAXX-1', 'T_DRAXX-2', 'T_DRAXX-3', 'T_DRAXX-4', 'T_DRAXX-5', 'T_DRAXX-6']} We'll quickly inspect how much of the current generator capacity our contextual plant data covers. flatten_list = lambda list_: [item for sublist in list_ for item in sublist] bmu_ids_with_metadata = sorted(list(set(flatten_list(osuked_id_to_bmu_ids.values())))) bmu_ids_with_metadata_and_output = df_PN.columns.intersection(bmu_ids_with_metadata) bmu_ids_without_metadata = sorted(list(set(df_PN.columns) -set(bmu_ids_with_metadata))) pct_site_coverage = len(bmu_ids_with_metadata_and_output)/df_PN.columns.size pct_output_coverage = df_PN.sum()[bmu_ids_with_metadata_and_output].sum()/df_PN.sum().sum() # print(f\"{pct_site_coverage:.0%} of the sites have coverage, making up {pct_output_coverage:.0%} of the total power output. The following are missing:\\n{', '.join(bmu_ids_without_metadata)}\") We'll now create a new dataframe that contains the contextual plant data as well as the generation time-series. #exports def extract_PN_ts(df_PN, bmu_ids, n_SPs=48*7): matching_output_bmu_ids = df_PN.columns.intersection(bmu_ids) output_match = matching_output_bmu_ids.size > 0 if output_match == False: return None s_PN = df_PN[matching_output_bmu_ids].sum(axis=1) s_PN.index = (s_PN.index.tz_convert('UTC') - pd.to_datetime(0, unit='s').tz_localize('UTC')).total_seconds().astype(int) * 1000 s_PN = s_PN.fillna(0) s_PN[s_PN<0] = 0 PN_ts = s_PN.tail(n_SPs).to_dict() return PN_ts def construct_map_df( df_PN, osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_lat_lon, osuked_id_to_fuel_type, osuked_id_to_name, n_SPs=48*7 ): sites_data = list() for osuked_id, bmu_ids in osuked_id_to_bmu_ids.items(): lat_lon_match = osuked_id in osuked_id_to_lat_lon.keys() PN_ts = extract_PN_ts(df_PN, bmu_ids, n_SPs=n_SPs) if lat_lon_match and PN_ts is not None: if sum(PN_ts.values()) > 0: site_data = osuked_id_to_lat_lon[osuked_id] site_data.update({'id': osuked_id}) site_data.update({'name': osuked_id_to_name[osuked_id]}) site_data.update({'capacity': osuked_id_to_capacity_mw[osuked_id]}) site_data.update({'fuel_type': osuked_id_to_fuel_type[osuked_id]}) site_data.update({'output': PN_ts}) sites_data += [site_data] df_map = pd.DataFrame(sites_data).set_index('id') return df_map df_map = construct_map_df(df_PN, osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_lat_lon, osuked_id_to_fuel_type, osuked_id_to_name) df_map.head(3) id latitude longitude name capacity fuel_type output 10000 57.4804 -3.60352 Rothes Bio-Plant CHP nan biomass {1624033800000: 55.0, 1624035600000: 55.0, 162... 10004 53.7487 -0.626221 Drax 1980 coal, biomass {1624033800000: 1950.0, 1624035600000: 1950.0,... 10010 55.2042 -1.52083 Lynemouth Generator nan coal {1624033800000: 405.0, 1624035600000: 405.0, 1... We now want to convert this into a geodataframe so we can visualise it spatially #exports def df_to_gdf(df, lat_col='latitude', lon_col='longitude'): geometry = gpd.points_from_xy(df[lon_col], df[lat_col]) gdf = gpd.GeoDataFrame(df, geometry=geometry) return gdf gdf_map = df_to_gdf(df_map) gdf_map_latest = gdf_map.assign(output=gdf_map['output'].apply(lambda d: list(d.values())[-1])) gdf_map_latest.plot(markersize='output', column='fuel_type', alpha=0.5, legend=True) <AxesSubplot:> #exports def construct_map_geojson( df_PN, df_powerdict, n_SPs=48*7 ): osuked_id_mappings = construct_osuked_id_mappings(df_powerdict) osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_fuel_type, osuked_id_to_name, osuked_id_to_lat_lon = osuked_id_mappings.values() df_map = construct_map_df(df_PN, osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_lat_lon, osuked_id_to_fuel_type, osuked_id_to_name, n_SPs=n_SPs) gdf_map = df_to_gdf(df_map) geojson = json.loads(gdf_map.to_json().replace('\"nan\"', 'null')) geojson['timeseries'] = [int(unix_datetime) for unix_datetime in list(geojson['features'][0]['properties']['output'].keys())] return geojson geojson = construct_map_geojson(df_PN, df_powerdict) JSON(geojson) <IPython.core.display.JSON object> #exports def save_map_geojson(geojson, fp='data/power_plants.json'): with open(fp, 'w') as f: json.dump(geojson, f) save_map_geojson(geojson, fp='../data/power_plants.json') #exports def get_nearest_dt_idx(geojson, nearest_half_hour): ts = pd.to_datetime([x*1000000 for x in geojson['timeseries']]).tz_localize('UTC').tz_convert('Europe/London') nearest_hh_match = [i for i, dt in enumerate(ts) if dt==nearest_half_hour] if len(nearest_hh_match) == 1: return nearest_hh_match[0] else: return len(ts)-1 def generate_map_js( df_PN: pd.DataFrame, df_powerdict: pd.DataFrame, zoom: int=5, center: list=[54.8, -4.61], js_template_fp: str='templates/map.js', js_docs_fp: str='docs/js/map.js', plants_geojson_fp: str='data/power_plants.json', plants_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/power_plants.json', routes_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/network_routes.json' ): geojson = construct_map_geojson(df_PN, df_powerdict) save_map_geojson(geojson, fp=plants_geojson_fp) nearest_half_hour = (pd.Timestamp.now().tz_localize('Europe/London')+pd.Timedelta(minutes=15)).round('30min') nearest_dt_idx = get_nearest_dt_idx(geojson, nearest_half_hour) rendered_map_js = Template(open(js_template_fp).read()).render( zoom=zoom, start_idx=nearest_dt_idx, center=center, plants_geojson_url=plants_geojson_url, routes_geojson_url=routes_geojson_url, ) with open(js_docs_fp, 'w', encoding='utf8') as fp: fp.write(rendered_map_js) return generate_map_js(df_PN, df_powerdict, plants_geojson_fp='../data/power_plants.json', js_template_fp='../templates/map.js', js_docs_fp='../docs/js/map.js') # clean up the pop-up #exports def generate_map_md(md_template_fp='templates/map.md', md_docs_fp='docs/map.md'): update_date = pd.Timestamp.now().round('5min').tz_localize('Europe/London').strftime('%Y-%m-%d %H:%M') rendered_map_md = Template(open(md_template_fp).read()).render({'update_date': update_date}) with open(md_docs_fp, 'w', encoding='utf8') as fp: fp.write(rendered_map_md) return generate_map_md(md_template_fp='../templates/map.md', md_docs_fp='../docs/map.md') #exports app = typer.Typer() python -m ElexonDataPortal.dev.mapgen #exports @app.command() def generate_map( api_key: str=None, powerdict_url: str='https://raw.githubusercontent.com/OSUKED/Power-Station-Dictionary/main/data/output/power_stations.csv', js_template_fp: str='templates/map.js', js_docs_fp: str='docs/js/map.js', md_template_fp: str='templates/map.md', md_docs_fp: str='docs/map.md', data_dir: str='data/PN', plants_geojson_fp: str='data/power_plants.json', plants_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/power_plants.json', routes_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/network_routes.json' ): if api_key is None: assert 'BMRS_API_KEY' in os.environ.keys(), 'If the `api_key` is not specified during client initialisation then it must be set to as the environment variable `BMRS_API_KEY`' api_key = os.environ['BMRS_API_KEY'] download_latest_PHYBM_data(api_key, data_dir) df_PN = load_most_recent_PN_data(data_dir) df_powerdict = pd.read_csv(powerdict_url) generate_map_js(df_PN, df_powerdict, js_template_fp=js_template_fp, js_docs_fp=js_docs_fp, plants_geojson_fp=plants_geojson_fp, plants_geojson_url=plants_geojson_url, routes_geojson_url=routes_geojson_url) generate_map_md(md_template_fp=md_template_fp, md_docs_fp=md_docs_fp) return generate_map( api_key=api_key, js_template_fp='../templates/map.js', js_docs_fp='../docs/js/map.js', md_template_fp='../templates/map.md', md_docs_fp='../docs/map.md', data_dir='../data/PN', plants_geojson_fp='../data/power_plants.json' ) #exports if __name__ == '__main__' and '__file__' in globals(): app()","title":"Map Generation"},{"location":"09-map-gen/#map-generation","text":"This notebook shows the development of the code required to generate the live map of energy generation by power plant.","title":"Map Generation"},{"location":"09-map-gen/#imports","text":"#exports import json import numpy as np import pandas as pd import geopandas as gpd import os import typer from tqdm import tqdm from jinja2 import Template from ElexonDataPortal.dev import utils, raw from IPython.display import JSON import matplotlib.pyplot as plt","title":"Imports"},{"location":"09-map-gen/#network-route-map","text":"The raw network route map data can be retrieved from here . We want to extract the relevant data, group the different cables/lines according to capacity, then save the information as a geojson. #exports def clean_route_gdf(gdf): s_LV_routes = gdf['OPERATING_'].astype(float).fillna(0)<=132 gdf.loc[s_LV_routes, 'OPERATING_'] = '<=132' gdf.loc[~s_LV_routes, 'OPERATING_'] = gdf.loc[~s_LV_routes, 'OPERATING_'].astype(int).astype(str) return gdf def load_route_gdf(data_dir='data'): gdf_OHL = gpd.read_file(f'{data_dir}/transmission-system/OHL.shp').to_crs('EPSG:4326') gdf_cables = gpd.read_file(f'{data_dir}/transmission-system/Cable.shp').to_crs('EPSG:4326') gdf_OHL = clean_route_gdf(gdf_OHL) gdf_cables = clean_route_gdf(gdf_cables) gdf_route = gdf_OHL.append(gdf_cables)[['OPERATING_', 'geometry']].rename(columns={'OPERATING_': 'kV'}) gdf_route.to_file(f'{data_dir}/network_routes.json', driver='GeoJSON') return gdf_route gdf_route = load_route_gdf('../data') gdf_route.head(3) kV geometry 400 LINESTRING Z (-2.02839 51.85078 0.00000, -2.02... 400 LINESTRING Z (-2.02754 51.84931 0.00000, -2.02... 400 LINESTRING Z (-3.12672 51.20292 0.00000, -3.12... We'll quickly visualise the network route and count how many of the different capacity types we have. gdf_route.plot(column='kV', legend=True, legend_kwds={'frameon': False}) gdf_route['kV'].value_counts() 275 1858 400 1232 <=132 1087 Name: kV, dtype: int64","title":"Network Route Map"},{"location":"09-map-gen/#power-plant-output-data-retrieval","text":"We'll begin by creating a function to retrieve and clean Physical Notification (PN) data for transmission level plants in Great Britain. #exports def construct_df_PN_pivot_dt_rng(df_PN): no_seconds = (((df_PN['timeFrom'].str.split(':').str[-1]=='00').mean()==1) & ((df_PN['timeTo'].str.split(':').str[-1]=='00').mean()==1)) if no_seconds == True: dt_rng = pd.date_range(df_PN['timeFrom'].min(), df_PN['timeTo'].max(), freq='min', tz='Europe/London') else: dt_rng = pd.date_range(df_PN['timeFrom'].min(), df_PN['timeTo'].max(), freq='s', tz='Europe/London') return dt_rng def construct_PN_pivot_df(df_PN, resample=None): bmu_ids = sorted(list(df_PN['bmUnitID'].unique())) df_PN_pivot = pd.DataFrame(index=construct_df_PN_pivot_dt_rng(df_PN), columns=bmu_ids, dtype=float) for bmu_id in tqdm(bmu_ids): for idx, row in df_PN.query('bmUnitID==@bmu_id').iterrows(): df_PN_pivot.loc[pd.to_datetime(row['timeFrom']).tz_localize('Europe/London'), bmu_id] = float(row['pnLevelFrom']) df_PN_pivot.loc[pd.to_datetime(row['timeTo']).tz_localize('Europe/London'), bmu_id] = float(row['pnLevelTo']) df_PN_pivot[bmu_id] = df_PN_pivot[bmu_id].interpolate() if resample is not None: df_PN_pivot = df_PN_pivot.resample(resample).mean() return df_PN_pivot def get_PHYBM_df(api_key, start_date=None, end_date=None, record_type='PN', resample='30T'): if start_date is None and end_date is None: start_date = pd.Timestamp.now().round('30min') - pd.Timedelta(minutes=60*1) end_date = pd.Timestamp.now().round('30min') + pd.Timedelta(minutes=180) elif start_date is not None and end_date is not None: pass else: raise ValueError('Only one of `start_date` and `end_date` was specified') df = pd.DataFrame() df_local_datetime_to_date_SP = utils.dt_rng_to_SPs(start_date, end_date) for idx, (date, SP) in tqdm(df_local_datetime_to_date_SP.iterrows(), total=df_local_datetime_to_date_SP.shape[0]): df_SP = utils.parse_xml_response(raw.get_PHYBMDATA(api_key, SettlementDate=date, SettlementPeriod=SP, ServiceType='xml')) df = df.append(df_SP) df = (df .query(f'recordType==\"{record_type}\"') .dropna(how='all', axis=1) ) if resample is not None: df = df.pipe(construct_PN_pivot_df, resample=resample) df.index.name = 'local_datetime' return df df_PN = get_PHYBM_df(api_key) df_PN.index.min(), df_PN.index.max() 44%\u00e2\u2013\u017d | 4/9 [00:13<00:15, 3.13s/it]c:\\users\\ayrto\\desktop\\phd\\data\\bmrs\\elexon-bmrs-api-wrapper\\ElexonDataPortal\\dev\\utils.py:29: UserWarning: Data request was succesful but no content was returned warn(f'Data request was succesful but no content was returned') 100% 9/9 [00:14<00:00, 1.58s/it] 100% 1370/1370 [00:10<00:00, 133.33it/s] (Timestamp('2021-06-25 15:00:00+0100', tz='Europe/London', freq='30T'), Timestamp('2021-06-25 17:00:00+0100', tz='Europe/London', freq='30T')) We'll now inspect the data from the closest half-hour settlement period that is available. nearest_half_hour = (pd.Timestamp.now(tz='Europe/London')+pd.Timedelta(minutes=15)).round('30min') most_recent_available_dt = max(df_PN.index[df_PN.index<=nearest_half_hour]) s_PN = df_PN.loc[most_recent_available_dt] s_PN.replace(0, np.nan).dropna().head() 2__ABGAS000 -378.0 2__AEELC000 -174.0 2__AEMEB000 -200.0 2__AENRD000 -145.0 2__AEOND000 -177.0 Name: 2021-06-25 17:00:00+01:00, dtype: float64 We want to be able to show not just current but historical data, however the time it takes to download the PN is quite long. To get around this issue we'll save data in monthly batches and only request data that is not currently contained in the existing csv files. #exports get_files = lambda data_dir: [f for f in os.listdir(data_dir) if '.csv' in f] def download_latest_PHYBM_data(api_key=None, data_dir='data/PN', record_type='PN'): if api_key is None: if 'BMRS_API_KEY' in os.environ.keys(): api_key = os.environ['BMRS_API_KEY'] else: raise ValueError('`api_key` must be passed or set as the environment variable `BMRS_API_KEY`') files = get_files(data_dir) years_months_downloaded = [f.split('.')[0] for f in files] current_ts = pd.Timestamp.now(tz='Europe/London') current_year_month = current_ts.strftime('%Y-%m') if current_year_month not in years_months_downloaded: start_date, end_date = f'{current_year_month}-01 00:00', current_ts.strftime('%Y-%m-%d %H:%M') df = get_PHYBM_df(api_key, start_date, end_date, record_type=record_type) df.to_csv(f'{data_dir}/{current_year_month}.csv') else: df = pd.read_csv(f'{data_dir}/{current_year_month}.csv') df = df.set_index('local_datetime') df.index = pd.to_datetime(df.index, utc=True).tz_convert('Europe/London') dt_rng = pd.date_range(df.index.max(), current_ts, freq='30T', tz='Europe/London') if dt_rng.size > 1: start_date = dt_rng[0] - pd.Timedelta(minutes=30) end_date = dt_rng[-1] + pd.Timedelta(minutes=60) try: df_latest = get_PHYBM_df(api_key, start_date, end_date, record_type=record_type) df_trimmed = df.drop(list(set(df_latest.index) - (set(df_latest.index) - set(df.index)))) df_combined = df_trimmed.append(df_latest).sort_index() df_combined.to_csv(f'{data_dir}/{current_year_month}.csv') except: warn(f'Could not retrieve any new data between {start_date} and {end_date}') download_latest_PHYBM_data(api_key, data_dir='../data/PN') 100% 5/5 [00:18<00:00, 3.78s/it] 100% 1370/1370 [00:11<00:00, 118.20it/s] We'll also create a helper function for loading in the latest data, when there is less than a week's worth of data in the latest month we'll append the dataframe to last months data as well. #exports def load_most_recent_PN_data(data_dir='data/PN', latest_year_month_file=None, df_PN_old=None): if latest_year_month_file is None: PN_files = sorted(get_files(data_dir)) latest_year_month_file = PN_files[-1] latest_fp = f'{data_dir}/{latest_year_month_file}' df_PN = pd.read_csv(latest_fp) df_PN['local_datetime'] = pd.to_datetime(df_PN['local_datetime'], utc=True) df_PN = df_PN.set_index('local_datetime') df_PN.index = df_PN.index.tz_convert('Europe/London') if df_PN_old is not None: assert latest_year_month_file is not None, 'Should not be appending to the main dataframe if `latest_year_month_file` was not specified' df_PN = df_PN_old.append(df_PN) if df_PN.shape[0] < (48*7): # want to have at least a week's worth of data assert 'PN_files' in locals(), 'The two most recent files combined have less than one week\\'s worth of data' df_PN = load_most_recent_PN_data(data_dir, latest_year_month_file=PN_files[-2], df_PN_old=df_PN) return df_PN df_PN = load_most_recent_PN_data(data_dir='../data/PN') df_PN.tail() local_datetime 2__AANGE001 2__AANGE002 2__ABGAS000 2__ACNDL001 2__AEDFE000 2__AEDIR000 2__AEELC000 2__AEENG000 2__AEMEB000 2__AENRD000 ... V__KGAZP002 V__LCEND001 V__LFLEX001 V__MADEL001 V__MGBLO001 V__NFLEX001 V__PFLEX001 I_I2D-MQBN1 I_I2G-MQBN1 I_IFG-MQBN1 2021-06-25 15:00:00+01:00 0 0 -299.917 0 0 0 -154.833 0 -177.8 -171.617 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 15:30:00+01:00 0 0 -335.4 0 0 0 -161.933 0 -188.35 -162.2 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 16:00:00+01:00 0 0 -365.6 0 0 0 -168.833 0 -196.383 -150.683 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 16:30:00+01:00 0 0 -378 0 0 0 -174 0 -200 -145 ... 0 0 0 0 0 0 0 0 0 0 2021-06-25 17:00:00+01:00 0 0 -378 0 0 0 -174 0 -200 -145 ... 0 0 0 0 0 0 0 0 0 0","title":"Power Plant Output Data Retrieval"},{"location":"09-map-gen/#location-fuel-type-data-collation","text":"Now we're redy to start building the map, for this we'll need information on both the location and fuel-type (which will be represented by the colour of the plant). We can get this data from the power station dictionary project. df_powerdict = pd.read_csv('https://raw.githubusercontent.com/OSUKED/Power-Station-Dictionary/main/data/output/power_stations.csv') df_powerdict.head(3) osuked_id esail_id gppd_idnr name sett_bmu_id longitude latitude fuel_type capacity_mw 10000 MARK nan Rothes Bio-Plant CHP E_MARK-1, E_MARK-2 -3.60352 57.4804 biomass nan 10001 DIDC nan Didcot A (G) T_DIDC1, T_DIDC2, T_DIDC4, T_DIDC3 -1.26757 51.6236 coal nan 10002 ABTH GBR1000374 Aberthaw B T_ABTH7, T_ABTH8, T_ABTH9 -3.40487 51.3873 coal 1586 We'll create some helper dictionaries for mapping from the osuked id to the data we're interested in #exports def construct_osuked_id_mappings(df_powerdict): osuked_id_mappings = dict() osuked_id_mappings['bmu_ids'] = (df_powerdict .set_index('osuked_id') ['sett_bmu_id'] .str.split(', ') .dropna() .to_dict() ) osuked_id_mappings['capacity_mw'] = (df_powerdict .set_index('osuked_id') ['capacity_mw'] .astype(str) .str.replace('.0', '', regex=False) .to_dict() ) osuked_id_mappings['fuel_type'] = (df_powerdict .set_index('osuked_id') ['fuel_type'] .dropna() .to_dict() ) osuked_id_mappings['name'] = (df_powerdict .set_index('osuked_id') ['name'] .dropna() .to_dict() ) osuked_id_mappings['lat_lon'] = (df_powerdict .set_index('osuked_id') [['latitude', 'longitude']] .dropna() .apply(dict, axis=1) .to_dict() ) return osuked_id_mappings osuked_id_mappings = construct_osuked_id_mappings(df_powerdict) osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_fuel_type, osuked_id_to_name, osuked_id_to_lat_lon = osuked_id_mappings.values() pd.Series(osuked_id_to_bmu_ids).head().to_dict() {10000: ['E_MARK-1', 'E_MARK-2'], 10001: ['T_DIDC1', 'T_DIDC2', 'T_DIDC4', 'T_DIDC3'], 10002: ['T_ABTH7', 'T_ABTH8', 'T_ABTH9'], 10003: ['T_COTPS-1', 'T_COTPS-2', 'T_COTPS-3', 'T_COTPS-4'], 10004: ['T_DRAXX-1', 'T_DRAXX-2', 'T_DRAXX-3', 'T_DRAXX-4', 'T_DRAXX-5', 'T_DRAXX-6']} We'll quickly inspect how much of the current generator capacity our contextual plant data covers. flatten_list = lambda list_: [item for sublist in list_ for item in sublist] bmu_ids_with_metadata = sorted(list(set(flatten_list(osuked_id_to_bmu_ids.values())))) bmu_ids_with_metadata_and_output = df_PN.columns.intersection(bmu_ids_with_metadata) bmu_ids_without_metadata = sorted(list(set(df_PN.columns) -set(bmu_ids_with_metadata))) pct_site_coverage = len(bmu_ids_with_metadata_and_output)/df_PN.columns.size pct_output_coverage = df_PN.sum()[bmu_ids_with_metadata_and_output].sum()/df_PN.sum().sum() # print(f\"{pct_site_coverage:.0%} of the sites have coverage, making up {pct_output_coverage:.0%} of the total power output. The following are missing:\\n{', '.join(bmu_ids_without_metadata)}\") We'll now create a new dataframe that contains the contextual plant data as well as the generation time-series. #exports def extract_PN_ts(df_PN, bmu_ids, n_SPs=48*7): matching_output_bmu_ids = df_PN.columns.intersection(bmu_ids) output_match = matching_output_bmu_ids.size > 0 if output_match == False: return None s_PN = df_PN[matching_output_bmu_ids].sum(axis=1) s_PN.index = (s_PN.index.tz_convert('UTC') - pd.to_datetime(0, unit='s').tz_localize('UTC')).total_seconds().astype(int) * 1000 s_PN = s_PN.fillna(0) s_PN[s_PN<0] = 0 PN_ts = s_PN.tail(n_SPs).to_dict() return PN_ts def construct_map_df( df_PN, osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_lat_lon, osuked_id_to_fuel_type, osuked_id_to_name, n_SPs=48*7 ): sites_data = list() for osuked_id, bmu_ids in osuked_id_to_bmu_ids.items(): lat_lon_match = osuked_id in osuked_id_to_lat_lon.keys() PN_ts = extract_PN_ts(df_PN, bmu_ids, n_SPs=n_SPs) if lat_lon_match and PN_ts is not None: if sum(PN_ts.values()) > 0: site_data = osuked_id_to_lat_lon[osuked_id] site_data.update({'id': osuked_id}) site_data.update({'name': osuked_id_to_name[osuked_id]}) site_data.update({'capacity': osuked_id_to_capacity_mw[osuked_id]}) site_data.update({'fuel_type': osuked_id_to_fuel_type[osuked_id]}) site_data.update({'output': PN_ts}) sites_data += [site_data] df_map = pd.DataFrame(sites_data).set_index('id') return df_map df_map = construct_map_df(df_PN, osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_lat_lon, osuked_id_to_fuel_type, osuked_id_to_name) df_map.head(3) id latitude longitude name capacity fuel_type output 10000 57.4804 -3.60352 Rothes Bio-Plant CHP nan biomass {1624033800000: 55.0, 1624035600000: 55.0, 162... 10004 53.7487 -0.626221 Drax 1980 coal, biomass {1624033800000: 1950.0, 1624035600000: 1950.0,... 10010 55.2042 -1.52083 Lynemouth Generator nan coal {1624033800000: 405.0, 1624035600000: 405.0, 1... We now want to convert this into a geodataframe so we can visualise it spatially #exports def df_to_gdf(df, lat_col='latitude', lon_col='longitude'): geometry = gpd.points_from_xy(df[lon_col], df[lat_col]) gdf = gpd.GeoDataFrame(df, geometry=geometry) return gdf gdf_map = df_to_gdf(df_map) gdf_map_latest = gdf_map.assign(output=gdf_map['output'].apply(lambda d: list(d.values())[-1])) gdf_map_latest.plot(markersize='output', column='fuel_type', alpha=0.5, legend=True) <AxesSubplot:> #exports def construct_map_geojson( df_PN, df_powerdict, n_SPs=48*7 ): osuked_id_mappings = construct_osuked_id_mappings(df_powerdict) osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_fuel_type, osuked_id_to_name, osuked_id_to_lat_lon = osuked_id_mappings.values() df_map = construct_map_df(df_PN, osuked_id_to_bmu_ids, osuked_id_to_capacity_mw, osuked_id_to_lat_lon, osuked_id_to_fuel_type, osuked_id_to_name, n_SPs=n_SPs) gdf_map = df_to_gdf(df_map) geojson = json.loads(gdf_map.to_json().replace('\"nan\"', 'null')) geojson['timeseries'] = [int(unix_datetime) for unix_datetime in list(geojson['features'][0]['properties']['output'].keys())] return geojson geojson = construct_map_geojson(df_PN, df_powerdict) JSON(geojson) <IPython.core.display.JSON object> #exports def save_map_geojson(geojson, fp='data/power_plants.json'): with open(fp, 'w') as f: json.dump(geojson, f) save_map_geojson(geojson, fp='../data/power_plants.json') #exports def get_nearest_dt_idx(geojson, nearest_half_hour): ts = pd.to_datetime([x*1000000 for x in geojson['timeseries']]).tz_localize('UTC').tz_convert('Europe/London') nearest_hh_match = [i for i, dt in enumerate(ts) if dt==nearest_half_hour] if len(nearest_hh_match) == 1: return nearest_hh_match[0] else: return len(ts)-1 def generate_map_js( df_PN: pd.DataFrame, df_powerdict: pd.DataFrame, zoom: int=5, center: list=[54.8, -4.61], js_template_fp: str='templates/map.js', js_docs_fp: str='docs/js/map.js', plants_geojson_fp: str='data/power_plants.json', plants_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/power_plants.json', routes_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/network_routes.json' ): geojson = construct_map_geojson(df_PN, df_powerdict) save_map_geojson(geojson, fp=plants_geojson_fp) nearest_half_hour = (pd.Timestamp.now().tz_localize('Europe/London')+pd.Timedelta(minutes=15)).round('30min') nearest_dt_idx = get_nearest_dt_idx(geojson, nearest_half_hour) rendered_map_js = Template(open(js_template_fp).read()).render( zoom=zoom, start_idx=nearest_dt_idx, center=center, plants_geojson_url=plants_geojson_url, routes_geojson_url=routes_geojson_url, ) with open(js_docs_fp, 'w', encoding='utf8') as fp: fp.write(rendered_map_js) return generate_map_js(df_PN, df_powerdict, plants_geojson_fp='../data/power_plants.json', js_template_fp='../templates/map.js', js_docs_fp='../docs/js/map.js') # clean up the pop-up #exports def generate_map_md(md_template_fp='templates/map.md', md_docs_fp='docs/map.md'): update_date = pd.Timestamp.now().round('5min').tz_localize('Europe/London').strftime('%Y-%m-%d %H:%M') rendered_map_md = Template(open(md_template_fp).read()).render({'update_date': update_date}) with open(md_docs_fp, 'w', encoding='utf8') as fp: fp.write(rendered_map_md) return generate_map_md(md_template_fp='../templates/map.md', md_docs_fp='../docs/map.md') #exports app = typer.Typer() python -m ElexonDataPortal.dev.mapgen #exports @app.command() def generate_map( api_key: str=None, powerdict_url: str='https://raw.githubusercontent.com/OSUKED/Power-Station-Dictionary/main/data/output/power_stations.csv', js_template_fp: str='templates/map.js', js_docs_fp: str='docs/js/map.js', md_template_fp: str='templates/map.md', md_docs_fp: str='docs/map.md', data_dir: str='data/PN', plants_geojson_fp: str='data/power_plants.json', plants_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/power_plants.json', routes_geojson_url: str='https://raw.githubusercontent.com/OSUKED/ElexonDataPortal/master/data/network_routes.json' ): if api_key is None: assert 'BMRS_API_KEY' in os.environ.keys(), 'If the `api_key` is not specified during client initialisation then it must be set to as the environment variable `BMRS_API_KEY`' api_key = os.environ['BMRS_API_KEY'] download_latest_PHYBM_data(api_key, data_dir) df_PN = load_most_recent_PN_data(data_dir) df_powerdict = pd.read_csv(powerdict_url) generate_map_js(df_PN, df_powerdict, js_template_fp=js_template_fp, js_docs_fp=js_docs_fp, plants_geojson_fp=plants_geojson_fp, plants_geojson_url=plants_geojson_url, routes_geojson_url=routes_geojson_url) generate_map_md(md_template_fp=md_template_fp, md_docs_fp=md_docs_fp) return generate_map( api_key=api_key, js_template_fp='../templates/map.js', js_docs_fp='../docs/js/map.js', md_template_fp='../templates/map.md', md_docs_fp='../docs/map.md', data_dir='../data/PN', plants_geojson_fp='../data/power_plants.json' ) #exports if __name__ == '__main__' and '__file__' in globals(): app()","title":"Location &amp; Fuel Type Data Collation"},{"location":"10-nbdev/","text":"NB-Dev Modification Imports #exports from fastcore.foundation import Config, Path from nbdev import export import os import re #exports _re_version = re.compile('^__version__\\s*=.*$', re.MULTILINE) def update_version(): \"Add or update `__version__` in the main `__init__.py` of the library\" fname = Config().path(\"lib_path\")/'__init__.py' if not fname.exists(): fname.touch() version = f'__version__ = \"{Config().version}\"' with open(fname, 'r') as f: code = f.read() if _re_version.search(code) is None: code = version + \"\\n\" + code else: code = _re_version.sub(version, code) with open(fname, 'w') as f: f.write(code) export.update_version = update_version update_version() #exports def add_init(path, contents=''): \"Add `__init__.py` in all subdirs of `path` containing python files if it's not there already\" for p,d,f in os.walk(path): for f_ in f: if f_.endswith('.py'): if not (Path(p)/'__init__.py').exists(): (Path(p)/'__init__.py').write_text('\\n'+contents) break def update_version(init_dir=None, extra_init_contents=''): \"Add or update `__version__` in the main `__init__.py` of the library\" version = Config().version version_str = f'__version__ = \"{version}\"' if init_dir is None: path = Config().path(\"lib_path\") else: path = Path(init_dir) fname = path/'__init__.py' if not fname.exists(): add_init(path, contents=extra_init_contents) code = f'{version_str}\\n{extra_init_contents}' with open(fname, 'w') as f: f.write(code) export.add_init = add_init export.update_version = update_version #exports def prepare_nbdev_module(extra_init_contents=''): export.reset_nbdev_module() export.update_version(extra_init_contents=extra_init_contents) export.update_baseurl() prepare_nbdev_module() #exports def notebook2script(fname=None, silent=False, to_dict=False, bare=False, extra_init_contents=''): \"Convert notebooks matching `fname` to modules\" # initial checks if os.environ.get('IN_TEST',0): return # don't export if running tests if fname is None: prepare_nbdev_module(extra_init_contents=extra_init_contents) files = export.nbglob(fname=fname) d = collections.defaultdict(list) if to_dict else None modules = export.create_mod_files(files, to_dict, bare=bare) for f in sorted(files): d = export._notebook2script(f, modules, silent=silent, to_dict=d, bare=bare) if to_dict: return d else: add_init(Config().path(\"lib_path\")) return notebook2script() Converted 00-documentation.ipynb. Converted 01-utils.ipynb. Converted 02-spec-gen.ipynb. Converted 03-raw-methods.ipynb. Converted 04-client-prep.ipynb. Converted 05-orchestrator.ipynb. Converted 06-client-gen.ipynb. Converted 07-cli-rebuild.ipynb. Converted 08-quick-start.ipynb. Converted 09-map-gen.ipynb. Converted 10-nbdev.ipynb. Converted Example Usage.ipynb. #exports def add_mod_extra_indices(mod, extra_modules_to_source): for extra_module, module_source in extra_modules_to_source.items(): extra_module_fp = export.Config().path(\"lib_path\")/extra_module with open(extra_module_fp, 'r') as text_file: extra_module_code = text_file.read() names = export.export_names(extra_module_code) mod.index.update({name: module_source for name in names}) return mod def add_mod_extra_modules(mod, extra_modules): extra_modules = [e for e in extra_modules if e not in mod.modules] mod.modules = sorted(mod.modules + extra_modules) return mod def add_extra_code_desc_to_mod( extra_modules_to_source = { 'api.py': '06-client-gen.ipynb', 'dev/raw.py': '03-raw-methods.ipynb' } ): mod = export.get_nbdev_module() mod = add_mod_extra_indices(mod, extra_modules_to_source) mod = add_mod_extra_modules(mod, extra_modules_to_source.keys()) export.save_nbdev_module(mod) return # add_extra_code_desc_to_mod()","title":"NB-Dev Modification"},{"location":"10-nbdev/#nb-dev-modification","text":"","title":"NB-Dev Modification"},{"location":"10-nbdev/#imports","text":"#exports from fastcore.foundation import Config, Path from nbdev import export import os import re #exports _re_version = re.compile('^__version__\\s*=.*$', re.MULTILINE) def update_version(): \"Add or update `__version__` in the main `__init__.py` of the library\" fname = Config().path(\"lib_path\")/'__init__.py' if not fname.exists(): fname.touch() version = f'__version__ = \"{Config().version}\"' with open(fname, 'r') as f: code = f.read() if _re_version.search(code) is None: code = version + \"\\n\" + code else: code = _re_version.sub(version, code) with open(fname, 'w') as f: f.write(code) export.update_version = update_version update_version() #exports def add_init(path, contents=''): \"Add `__init__.py` in all subdirs of `path` containing python files if it's not there already\" for p,d,f in os.walk(path): for f_ in f: if f_.endswith('.py'): if not (Path(p)/'__init__.py').exists(): (Path(p)/'__init__.py').write_text('\\n'+contents) break def update_version(init_dir=None, extra_init_contents=''): \"Add or update `__version__` in the main `__init__.py` of the library\" version = Config().version version_str = f'__version__ = \"{version}\"' if init_dir is None: path = Config().path(\"lib_path\") else: path = Path(init_dir) fname = path/'__init__.py' if not fname.exists(): add_init(path, contents=extra_init_contents) code = f'{version_str}\\n{extra_init_contents}' with open(fname, 'w') as f: f.write(code) export.add_init = add_init export.update_version = update_version #exports def prepare_nbdev_module(extra_init_contents=''): export.reset_nbdev_module() export.update_version(extra_init_contents=extra_init_contents) export.update_baseurl() prepare_nbdev_module() #exports def notebook2script(fname=None, silent=False, to_dict=False, bare=False, extra_init_contents=''): \"Convert notebooks matching `fname` to modules\" # initial checks if os.environ.get('IN_TEST',0): return # don't export if running tests if fname is None: prepare_nbdev_module(extra_init_contents=extra_init_contents) files = export.nbglob(fname=fname) d = collections.defaultdict(list) if to_dict else None modules = export.create_mod_files(files, to_dict, bare=bare) for f in sorted(files): d = export._notebook2script(f, modules, silent=silent, to_dict=d, bare=bare) if to_dict: return d else: add_init(Config().path(\"lib_path\")) return notebook2script() Converted 00-documentation.ipynb. Converted 01-utils.ipynb. Converted 02-spec-gen.ipynb. Converted 03-raw-methods.ipynb. Converted 04-client-prep.ipynb. Converted 05-orchestrator.ipynb. Converted 06-client-gen.ipynb. Converted 07-cli-rebuild.ipynb. Converted 08-quick-start.ipynb. Converted 09-map-gen.ipynb. Converted 10-nbdev.ipynb. Converted Example Usage.ipynb. #exports def add_mod_extra_indices(mod, extra_modules_to_source): for extra_module, module_source in extra_modules_to_source.items(): extra_module_fp = export.Config().path(\"lib_path\")/extra_module with open(extra_module_fp, 'r') as text_file: extra_module_code = text_file.read() names = export.export_names(extra_module_code) mod.index.update({name: module_source for name in names}) return mod def add_mod_extra_modules(mod, extra_modules): extra_modules = [e for e in extra_modules if e not in mod.modules] mod.modules = sorted(mod.modules + extra_modules) return mod def add_extra_code_desc_to_mod( extra_modules_to_source = { 'api.py': '06-client-gen.ipynb', 'dev/raw.py': '03-raw-methods.ipynb' } ): mod = export.get_nbdev_module() mod = add_mod_extra_indices(mod, extra_modules_to_source) mod = add_mod_extra_modules(mod, extra_modules_to_source.keys()) export.save_nbdev_module(mod) return # add_extra_code_desc_to_mod()","title":"Imports"},{"location":"api/","text":"","title":"API"},{"location":"map/","text":"Map Power Output Last Updated: 2021-07-05 16:20 This map shows the power output of individual plants connected to the transmission grid as stated in their most recent Physical Notifications. This data can be retrieved through the PHYBMDATA BMRS stream, the out-turn data can be viewed one week later through the B1610 stream.","title":"Map"},{"location":"map/#map","text":"","title":"Map"},{"location":"map/#power-output","text":"Last Updated: 2021-07-05 16:20 This map shows the power output of individual plants connected to the transmission grid as stated in their most recent Physical Notifications. This data can be retrieved through the PHYBMDATA BMRS stream, the out-turn data can be viewed one week later through the B1610 stream.","title":"Power Output"},{"location":"visualisations/","text":"Visualisations On this page you can view visualisations of key phenomena in the GB power sector, ranging from long-term trends in the generation-mix and market prices to information on excess capacity in the grid. All data used in these visualisations was either sourced directly from BMRS using the ElexonDataPortal client, or has been derived from BMRS data streams. As with the other components of the ElexonDataPortal the code to generate these visualisations is open-source and users are welcome to contribute their own visualisations, for more detail on how to do this please refer to the user contribution guide Measuring the Progress and Impacts of Decarbonising British Electricity The figures shown here are attempts to replicate the visualisations from this paper by Dr Iain Staffell which finds that: CO2 emissions from British electricity have fallen 46% in the three years to June 2016. Emissions from imports and biomass are not attributed to electricity, but add 5%. Coal capacity fell 50% and output 75% due to prices, competition and legislation. Wind, solar and biomass provided 20% of demand in 2015, with a peak of 45%. Prices have become more volatile and net demand is falling towards must-run nuclear. These figures will be updated on a weekly basis, the last update was at: 2021-07-12 23:40 Weekly Averaged Generation Mix The following figure shows a stacked plot of the generation from different fuel types over time, averaged on a weekly basis. The original plot can be found here , the following description is taken directly from the paper. Over this period fossil fuels have become increasingly squeezed by the growth of imports, biomass, wind and solar. Coal is seen responding to seasonal changes in demand, and displaced gas over the second half of 2011. Gas generation fell steadily from an average of 17.3 GW in 2009\u201310 to just 9.3 GW in 2012\u201313. This trend reversed over the course of 2015 with gas generation rising from an average of 9.0 GW in the first quarter of 2015 to 13.8 GW in the first quarter of 2016. By May 2016 coal generation fell to an average of just 1.1 GW, and on the 10th of May instantaneous coal output fell to zero for the first in over 130 years. Marginal Price Curve Estimation The figure shown here is reproduced from the work in this paper by Ayrton Bourn which investigates the merit order effect of renewables in the GB and DE power markets in terms of the price and carbon reductions - the key findings are as follows: A LOWESS estimation of the non-linear marginal price curve for dispatchable generation shows high back-casting accuracy for Germany and Britain The evolving Merit Order Effect (MOE) was estimated through a time-adaptive model, enabling long-term trends to be captured In Britain the MOE has increased sharply since 2016, with a 0.67% price reduction per p.p. increase in RES penetration Disaggregation of the MOE by fuel-type highlights key differences in the transition paths of Britain and Germany This figure will be updated on a weekly basis, the last update was at: 2021-07-12 23:40 Smoothed Price Time-Series Surface This figure shows the LOWESS (Locally Weighted Scatterplot Smoothing) regressions for the day-ahead marginal price curve visualised as a heatmap surface, highlighting the seasonal and non-cyclical changes over time. A mask has been applied where the residual demand after RES is outside the range of 99% of the data. This view is particularly helpful for picking up long-term trends in the market, for example the higher power prices seen in 18/19 due to high gas prices. De-Rated Margin In each settlement period the system operator publishes the de-rated margin forecast calculated in accordance with the Loss of Load Probability Calculation Statement at the following times: At 1200 hours on each calendar day for all Settlement Periods for which Gate Closure has not yet passed and which occur within the current Operational Day or the following Operational Day; and At eight, four, two and one hour(s) prior to the beginning of the Settlement Period to which the De-Rated Margin Forecast relates. These figures will be updated on an hourly basis, the last update was at: 2021-07-17 13:15 Forecasts The following heatmap shows the evolving de-rated margin forecast across the different forecast horizons. Forecast Deltas The following heatmap shows how the more recent de-rated margin forecasts deviate from the 8 hours ahead forecast. Power Output Map Last Updated: 2021-07-17 13:20 This map shows the power output of individual plants connected to the transmission grid as stated in their most recent Physical Notifications. This data can be retrieved through the PHYBMDATA BMRS stream, the out-turn data can be viewed one week later through the B1610 stream. The scrollbar at the top of the map can be used to view how the generation sources have changed their output over the last week, the layer control can be used to toggle on/off the transmission network, and the plants themselves can be clicked to reveal additional information. The plant location data used in this map has been taking from a sister project, the Power Station Dictionary , which aims to link between data about individual power plants. Contributor Guide We encourage users to contribute their own visualisations which the ElexonDataPortal will then update automatically. To this end the library adopts a standardised format for generating visualisations, the core component of which is the data/vis_configs.json file to which you will have to add detail on your visualisation function: [ ... { \"cron\": \"0 * * * *\", # the update schedule, in this instance to run at midnight every sunday \"function\": \"path_to_function\", # e.g. ElexonDataPortal.vis.generate_vis \"kwargs\": { 'api_key': null, # if no api_key is passed then the client will try and look for the `BMRS_API_KEY` environment variable 'update_time': null, # if no update_time is passed you should generate it yourself, e.g. with `pd.Timestamp.now().round('5min').strftime('%Y-%m-%d %H:%M')` 'docs_dir': 'docs', # in almost all circumstances this should just be `docs` \"optional_kwarg\": \"optional_value\" # you can specify any additional keyword arguments that your function requires } }, ... ] The other core component is writing the function that generates the visualisation. This function should require parameters for the docs_dir , api_key , and update_time but can include optional parameters that you wish to specify, it should then return markdown text which will be used to populate the Visualisations page. These functions will normally contain three steps: data retrieval, generating the visualisation, and generating the accompanying text - an example can be seen below. import pandas as pd import matplotlib.pyplot as plt from ..api import Client def generate_vis( docs_dir: str='docs', api_key: str=None, update_time: str=pd.Timestamp.now().round('5min').strftime('%Y-%m-%d %H:%M'), ) -> str: # Data Retrieval client = Client(api_key=api_key) df = client.get_data_stream(param1, param2) # Generating the Visualisation fig, ax = plt.subplots(dpi=150) df.plot(ax=ax) fig.savefig(f'{docs_dir}/img/vis/great_vis_name.png') # Generating the Text md_text = f\"\"\"### Title Explanation of what your visualisation shows ![](img/vis/great_vis_name.png) \"\"\" return md_text N.b. the path to the image should be relative to the docs directory. If you require any assistance in this process please start a discussion here and we'll endeavour to help as best we can.","title":"Visualisations"},{"location":"visualisations/#visualisations","text":"On this page you can view visualisations of key phenomena in the GB power sector, ranging from long-term trends in the generation-mix and market prices to information on excess capacity in the grid. All data used in these visualisations was either sourced directly from BMRS using the ElexonDataPortal client, or has been derived from BMRS data streams. As with the other components of the ElexonDataPortal the code to generate these visualisations is open-source and users are welcome to contribute their own visualisations, for more detail on how to do this please refer to the user contribution guide","title":"Visualisations"},{"location":"visualisations/#measuring-the-progress-and-impacts-of-decarbonising-british-electricity","text":"The figures shown here are attempts to replicate the visualisations from this paper by Dr Iain Staffell which finds that: CO2 emissions from British electricity have fallen 46% in the three years to June 2016. Emissions from imports and biomass are not attributed to electricity, but add 5%. Coal capacity fell 50% and output 75% due to prices, competition and legislation. Wind, solar and biomass provided 20% of demand in 2015, with a peak of 45%. Prices have become more volatile and net demand is falling towards must-run nuclear. These figures will be updated on a weekly basis, the last update was at: 2021-07-12 23:40","title":"Measuring the Progress and Impacts of Decarbonising British Electricity"},{"location":"visualisations/#weekly-averaged-generation-mix","text":"The following figure shows a stacked plot of the generation from different fuel types over time, averaged on a weekly basis. The original plot can be found here , the following description is taken directly from the paper. Over this period fossil fuels have become increasingly squeezed by the growth of imports, biomass, wind and solar. Coal is seen responding to seasonal changes in demand, and displaced gas over the second half of 2011. Gas generation fell steadily from an average of 17.3 GW in 2009\u201310 to just 9.3 GW in 2012\u201313. This trend reversed over the course of 2015 with gas generation rising from an average of 9.0 GW in the first quarter of 2015 to 13.8 GW in the first quarter of 2016. By May 2016 coal generation fell to an average of just 1.1 GW, and on the 10th of May instantaneous coal output fell to zero for the first in over 130 years.","title":"Weekly Averaged Generation Mix"},{"location":"visualisations/#marginal-price-curve-estimation","text":"The figure shown here is reproduced from the work in this paper by Ayrton Bourn which investigates the merit order effect of renewables in the GB and DE power markets in terms of the price and carbon reductions - the key findings are as follows: A LOWESS estimation of the non-linear marginal price curve for dispatchable generation shows high back-casting accuracy for Germany and Britain The evolving Merit Order Effect (MOE) was estimated through a time-adaptive model, enabling long-term trends to be captured In Britain the MOE has increased sharply since 2016, with a 0.67% price reduction per p.p. increase in RES penetration Disaggregation of the MOE by fuel-type highlights key differences in the transition paths of Britain and Germany This figure will be updated on a weekly basis, the last update was at: 2021-07-12 23:40","title":"Marginal Price Curve Estimation"},{"location":"visualisations/#smoothed-price-time-series-surface","text":"This figure shows the LOWESS (Locally Weighted Scatterplot Smoothing) regressions for the day-ahead marginal price curve visualised as a heatmap surface, highlighting the seasonal and non-cyclical changes over time. A mask has been applied where the residual demand after RES is outside the range of 99% of the data. This view is particularly helpful for picking up long-term trends in the market, for example the higher power prices seen in 18/19 due to high gas prices.","title":"Smoothed Price Time-Series Surface"},{"location":"visualisations/#de-rated-margin","text":"In each settlement period the system operator publishes the de-rated margin forecast calculated in accordance with the Loss of Load Probability Calculation Statement at the following times: At 1200 hours on each calendar day for all Settlement Periods for which Gate Closure has not yet passed and which occur within the current Operational Day or the following Operational Day; and At eight, four, two and one hour(s) prior to the beginning of the Settlement Period to which the De-Rated Margin Forecast relates. These figures will be updated on an hourly basis, the last update was at: 2021-07-17 13:15","title":"De-Rated Margin"},{"location":"visualisations/#forecasts","text":"The following heatmap shows the evolving de-rated margin forecast across the different forecast horizons.","title":"Forecasts"},{"location":"visualisations/#forecast-deltas","text":"The following heatmap shows how the more recent de-rated margin forecasts deviate from the 8 hours ahead forecast.","title":"Forecast Deltas"},{"location":"visualisations/#power-output-map","text":"Last Updated: 2021-07-17 13:20 This map shows the power output of individual plants connected to the transmission grid as stated in their most recent Physical Notifications. This data can be retrieved through the PHYBMDATA BMRS stream, the out-turn data can be viewed one week later through the B1610 stream. The scrollbar at the top of the map can be used to view how the generation sources have changed their output over the last week, the layer control can be used to toggle on/off the transmission network, and the plants themselves can be clicked to reveal additional information. The plant location data used in this map has been taking from a sister project, the Power Station Dictionary , which aims to link between data about individual power plants.","title":"Power Output Map"},{"location":"visualisations/#contributor-guide","text":"We encourage users to contribute their own visualisations which the ElexonDataPortal will then update automatically. To this end the library adopts a standardised format for generating visualisations, the core component of which is the data/vis_configs.json file to which you will have to add detail on your visualisation function: [ ... { \"cron\": \"0 * * * *\", # the update schedule, in this instance to run at midnight every sunday \"function\": \"path_to_function\", # e.g. ElexonDataPortal.vis.generate_vis \"kwargs\": { 'api_key': null, # if no api_key is passed then the client will try and look for the `BMRS_API_KEY` environment variable 'update_time': null, # if no update_time is passed you should generate it yourself, e.g. with `pd.Timestamp.now().round('5min').strftime('%Y-%m-%d %H:%M')` 'docs_dir': 'docs', # in almost all circumstances this should just be `docs` \"optional_kwarg\": \"optional_value\" # you can specify any additional keyword arguments that your function requires } }, ... ] The other core component is writing the function that generates the visualisation. This function should require parameters for the docs_dir , api_key , and update_time but can include optional parameters that you wish to specify, it should then return markdown text which will be used to populate the Visualisations page. These functions will normally contain three steps: data retrieval, generating the visualisation, and generating the accompanying text - an example can be seen below. import pandas as pd import matplotlib.pyplot as plt from ..api import Client def generate_vis( docs_dir: str='docs', api_key: str=None, update_time: str=pd.Timestamp.now().round('5min').strftime('%Y-%m-%d %H:%M'), ) -> str: # Data Retrieval client = Client(api_key=api_key) df = client.get_data_stream(param1, param2) # Generating the Visualisation fig, ax = plt.subplots(dpi=150) df.plot(ax=ax) fig.savefig(f'{docs_dir}/img/vis/great_vis_name.png') # Generating the Text md_text = f\"\"\"### Title Explanation of what your visualisation shows ![](img/vis/great_vis_name.png) \"\"\" return md_text N.b. the path to the image should be relative to the docs directory. If you require any assistance in this process please start a discussion here and we'll endeavour to help as best we can.","title":"Contributor Guide"}]}